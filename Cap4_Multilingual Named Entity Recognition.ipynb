{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f44346b8",
   "metadata": {},
   "source": [
    "**Tabla de contenido**\n",
    "\n",
    "- [The Dataset](#The-Dataset)\n",
    "- [Multilingual Transformers](#Multilingual-Transformers)\n",
    "- [Una mirada más cercana a la tokenización](#Una-mirada-mas-cercana-a-la-tokenizacion)\n",
    "     - [The Tokenizer Pipeline](#The-Tokenizer-Pipeline)\n",
    "     - [El tokenizador SentencePiece](#El-tokenizador-SentencePiece)\n",
    "- [Transformadores para Reconocimiento de Entidades Nombradas](#Transformadores-para-Reconocimiento-de-Entidades-Nombradas)\n",
    "- [The Anatomy of the Transformers Model Class](#The-Anatomy-of-the-Transformers-Model-Class)\n",
    "     - [Bodies and Heads](#Bodies-and-Heads)\n",
    "     - [Creando un modelo personalizado para clasificación de tokens](#Creando-un-modelo-personalizado-para-clasificacion-de-tokens)\n",
    "     - [Cargando un modelo personalizado](#Cargando-un-modelo-personalizado)\n",
    "- [Tokenizando textos para el NER](#Tokenizando-textos-para-el-NER)\n",
    "     - [Medidas de desempeño](#Medidas-de-desempeño)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926ce0da",
   "metadata": {},
   "source": [
    "Hasta ahora en este libro hemos aplicado transformadores para resolver tareas de NLP en corpora en inglés, pero `¿qué haces cuando tus documentos están escritos en griego, swahili o klingon?` Un enfoque es buscar en el Hugging Face Hub un modelo de lenguaje preentrenado adecuado y ajustarlo para la tarea en cuestión. Sin embargo, estos modelos preentrenados tienden a existir solo para lenguajes de \"alto recurso\" como alemán, ruso o mandarín, donde hay mucho texto web disponible para el preentrenamiento. Otro desafío común surge cuando tu corpus es multilingüe: mantener múltiples modelos monolingües en producción no será divertido ni para ti ni para tu equipo de ingeniería.\n",
    "\n",
    "Afortunadamente, existe una clase de transformadores multilingües que vienen al rescate. Al igual que BERT, estos modelos utilizan el modelado de lenguaje enmascarado como objetivo de preentrenamiento, pero se entrenan conjuntamente en textos en más de cien idiomas. Al preentrenarse en grandes corpus a través de muchos idiomas, estos transformadores multilingües permiten la transferencia cruzada de cero disparos. Esto significa que un modelo que se ajusta finamente a un idioma puede aplicarse a otros sin ningún entrenamiento adicional. ¡Esto también hace que estos modelos sean muy adecuados para el 'cambio de código', donde un hablante alterna entre dos o más idiomas o dialectos en el contexto de una sola conversación!\n",
    "\n",
    "En este capítulo exploraremos cómo un único modelo de transformador llamado XLM-RoBERTa (introducido en el Capítulo 3) puede ser ajustado para realizar el reconocimiento de entidades nombradas (NER) en varios idiomas. Como vimos en el Capítulo 1, el NER es una tarea común de procesamiento del lenguaje natural que identifica entidades como personas, organizaciones o ubicaciones en el texto. Estas entidades pueden ser utilizadas para diversas aplicaciones, como obtener información de documentos de empresas, mejorar la calidad de los motores de búsqueda, o simplemente construir una base de datos estructurada a partir de un corpus.\n",
    "\n",
    "Para este capítulo, supongamos que queremos realizar NER para un cliente con sede en Suiza, donde hay cuatro idiomas nacionales (siendo el inglés a menudo un puente entre ellos). Empecemos obteniendo un corpus multilingüe adecuado para este problema.\n",
    "\n",
    "Nota: `la Zero-shot transfer`  o el `zero-shot learning` generalmente se refiere a la tarea de entrenar un modelo en un conjunto de etiquetas y luego evaluarlo en un conjunto diferente de etiquetas. En el contexto de los transformadores, el zero-shot learning también puede referirse a situaciones en las que un modelo de lenguaje como GPT-3 es evaluado en una tarea posterior en la que ni siquiera se ha ajustado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157395a3",
   "metadata": {},
   "source": [
    "# The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08394d8f",
   "metadata": {},
   "source": [
    "En este capítulo, utilizaremos un subconjunto del benchmark de Evaluación de Transferencia Multilingüe de Encoders (XTREME) llamado WikiANN o PAN-X. Este conjunto de datos consiste en artículos de Wikipedia en muchos idiomas, incluidos los cuatro idiomas más hablados en Suiza: alemán (62.9%), francés (22.9%), italiano (8.4%) e inglés (5.9%).\n",
    "\n",
    "Cada artículo está anotado con etiquetas LOC (ubicación), PER (persona) y ORG (organización) en el formato \"dentro-fuera-comienzo\" (IOB2). En este formato, un prefijo B indica el comienzo de una entidad, y los tokens consecutivos que pertenecen a la misma entidad reciben un prefijo I-. Una etiqueta O indica que el token no pertenece a ninguna entidad. Por ejemplo, la siguiente oración:\n",
    "\n",
    " `Jeff Dean is a computer scientist at Google in California` \n",
    " \n",
    " se etiquetaría en formato IOB2 como se muestra en la siguiente Tabla.\n",
    "\n",
    "![Tabla](images/table4.png)\n",
    "\n",
    "Para cargar uno de los subconjuntos de PAN-X en XTREME, necesitaremos saber qué configuración de conjunto de datos pasar a la función load_dataset(). Siempre que trabajes con un conjunto de datos que tiene múltiples dominios, puedes usar la función get_dataset_config_names() para averiguar qué subconjuntos están disponibles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1476fdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XTREME has 183 configurations\n"
     ]
    }
   ],
   "source": [
    "from datasets import get_dataset_config_names\n",
    "xtreme_subsets = get_dataset_config_names(\"xtreme\")\n",
    "print(f\"XTREME has {len(xtreme_subsets)} configurations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12959e0",
   "metadata": {},
   "source": [
    "Vaya, ¡eso es un montón de configuraciones! Vamos a reducir la búsqueda buscando solo las configuraciones que comienzan con \"PAN\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccc0679f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PAN-X.af', 'PAN-X.ar', 'PAN-X.bg']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panx_subsets = [s for s in xtreme_subsets if s.startswith(\"PAN\")]\n",
    "panx_subsets[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b406cba5",
   "metadata": {},
   "source": [
    "De acuerdo, parece que hemos identificado la sintaxis de los subconjuntos de PAN-X: cada uno tiene un sufijo de dos letras que parece ser un código de idioma ISO 639-1. Esto significa que para cargar el corpus en alemán, pasamos el código al argumento name de load_dataset() de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c063ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "load_dataset(\"xtreme\", name=\"PAN-X.de\") # Cargar un dataset PAN-X para un idioma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8cc578",
   "metadata": {},
   "source": [
    "Para crear un corpus suizo realista, muestreamos los corpora de alemán (de), francés (fr), italiano (it) y inglés (en) de PAN-X de acuerdo con sus proporciones habladas. Esto creará un desequilibrio lingüístico que es muy común en conjuntos de datos del mundo real, donde adquirir ejemplos etiquetados en una lengua minoritaria puede ser costoso debido a la falta de expertos en la materia que hablen con fluidez ese idioma. Este conjunto de datos desequilibrado simulará una situación común al trabajar en aplicaciones multilingües, y veremos cómo podemos construir un modelo que funcione en todos los idiomas.\n",
    "\n",
    "Para llevar un registro de cada idioma, vamos a crear un defaultdict de Python que almacene el código del idioma como clave y un corpus PAN-X del tipo DatasetDict como valor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec3851ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Carga los datasets PAN-X para 4 idiomas (alemán, francés, italiano, inglés) y \n",
    "toma solo una fracción específica de los datos para balancearlos\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from datasets import DatasetDict\n",
    "\n",
    "langs = [\"de\", \"fr\", \"it\", \"en\"]\n",
    "fracs = [0.629, 0.229, 0.084, 0.059]\n",
    "panx_ch = defaultdict(DatasetDict)\n",
    "\n",
    "for lang, frac in zip(langs, fracs):\n",
    "    # Load monolingual corpus\n",
    "    ds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n",
    "    for split in ds:\n",
    "        panx_ch[lang][split] = (\n",
    "        ds[split].shuffle(seed=0).select(range(int(frac * ds[split].num_rows)))\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdae920a",
   "metadata": {},
   "source": [
    "Aquí hemos utilizado el método shuffle() para asegurarnos de que no sesgamos accidentalmente nuestras divisiones de conjuntos de datos, mientras que select() nos permite reducir la muestra de cada corpus de acuerdo con los valores en fracs. Echemos un vistazo a cuántos ejemplos tenemos por idioma en los conjuntos de entrenamiento accediendo al atributo Dataset.num_rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0e37b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>de</th>\n",
       "      <th>fr</th>\n",
       "      <th>it</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Number of training examples</th>\n",
       "      <td>12580</td>\n",
       "      <td>4580</td>\n",
       "      <td>1680</td>\n",
       "      <td>1180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                de    fr    it    en\n",
       "Number of training examples  12580  4580  1680  1180"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame({lang: [panx_ch[lang][\"train\"].num_rows] for lang in langs},index=[\"Number of training examples\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f00aae0",
   "metadata": {},
   "source": [
    "Por diseño, tenemos más ejemplos en alemán que en todos los demás idiomas combinados, por lo que lo usaremos como punto de partida para realizar una transferencia cruzada multilingüe de cero disparos al francés, italiano e inglés. Inspeccionemos uno de los ejemplos en el corpus en alemán:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d17df3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: ['2.000', 'Einwohnern', 'an', 'der', 'Danziger', 'Bucht', 'in', 'der', 'polnischen', 'Woiwodschaft', 'Pommern', '.']\n",
      "ner_tags: [0, 0, 0, 0, 5, 6, 0, 0, 5, 5, 6, 0]\n",
      "langs: ['de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de']\n"
     ]
    }
   ],
   "source": [
    "element = panx_ch[\"de\"][\"train\"][0]\n",
    "for key, value in element.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ecbda9",
   "metadata": {},
   "source": [
    "Al igual que en nuestros encuentros previos con objetos Dataset, las claves de nuestro ejemplo corresponden a los nombres de las columnas de una tabla Arrow, mientras que los valores denotan las entradas en cada columna. En particular, vemos que la columna ner_tags corresponde al mapeo de cada entidad a un ID de clase. Esto es un poco críptico para el ojo humano, así que vamos a crear una nueva columna con las etiquetas familiares LOC, PER y ORG. Para hacer esto, lo primero a notar es que nuestro objeto Dataset tiene un atributo de características que especifica los tipos de datos subyacentes asociados con cada columna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8851f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)\n",
      "ner_tags: Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None), length=-1, id=None)\n",
      "langs: Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)\n"
     ]
    }
   ],
   "source": [
    "for key, value in panx_ch[\"de\"][\"train\"].features.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775a84d1",
   "metadata": {},
   "source": [
    "La clase Sequence especifica que el campo contiene una lista de características, que en el caso de ner_tags corresponde a una lista de características ClassLabel. Seleccionemos esta característica del conjunto de entrenamiento de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94ace59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None)\n"
     ]
    }
   ],
   "source": [
    "tags = panx_ch[\"de\"][\"train\"].features[\"ner_tags\"].feature\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1721937e",
   "metadata": {},
   "source": [
    "Podemos usar el método ClassLabel.int2str() que encontramos en el Capítulo 2 para crear una nueva columna en nuestro conjunto de entrenamiento con los nombres de clase para cada etiqueta. Usaremos el método map() para devolver un diccionario con la clave correspondiente al nuevo nombre de columna y el valor como una lista de nombres de clase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb659978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tag_names(batch):\n",
    "    return {\"ner_tags_str\": [tags.int2str(idx) for idx in batch[\"ner_tags\"]]}\n",
    "panx_de = panx_ch[\"de\"].map(create_tag_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc7362e",
   "metadata": {},
   "source": [
    "Ahora que tenemos nuestras etiquetas en un formato legible para humanos, veamos cómo se alinean los tokens y las etiquetas para el primer ejemplo en el conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9099c082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>2.000</td>\n",
       "      <td>Einwohnern</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Danziger</td>\n",
       "      <td>Bucht</td>\n",
       "      <td>in</td>\n",
       "      <td>der</td>\n",
       "      <td>polnischen</td>\n",
       "      <td>Woiwodschaft</td>\n",
       "      <td>Pommern</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0           1   2    3         4      5   6    7           8   \\\n",
       "Tokens  2.000  Einwohnern  an  der  Danziger  Bucht  in  der  polnischen   \n",
       "Tags        O           O   O    O     B-LOC  I-LOC   O    O       B-LOC   \n",
       "\n",
       "                  9        10 11  \n",
       "Tokens  Woiwodschaft  Pommern  .  \n",
       "Tags           B-LOC    I-LOC  O  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_example = panx_de[\"train\"][0]\n",
    "pd.DataFrame([de_example[\"tokens\"], de_example[\"ner_tags_str\"]],\n",
    "['Tokens', 'Tags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addb07bd",
   "metadata": {},
   "source": [
    "La presencia de las etiquetas LOC tiene sentido ya que la frase \"2,000 Einwohnern an der Danziger Bucht in der polnischen Woiwodschaft Pommern\" significa \"2,000 habitantes en la bahía de Gdansk en la voivodía polaca de Pomerania\" en inglés, y la bahía de Gdansk es una bahía en el mar Báltico, mientras que \"voivodía\" corresponde a un estado en Polonia.\n",
    "\n",
    "Como una verificación rápida de que no tenemos ningún desequilibrio inusual en las etiquetas, calculemos las frecuencias de cada entidad en cada división:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87fa701a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOC</th>\n",
       "      <th>ORG</th>\n",
       "      <th>PER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>6186</td>\n",
       "      <td>5366</td>\n",
       "      <td>5810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation</th>\n",
       "      <td>3172</td>\n",
       "      <td>2683</td>\n",
       "      <td>2893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>3180</td>\n",
       "      <td>2573</td>\n",
       "      <td>3071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             LOC   ORG   PER\n",
       "train       6186  5366  5810\n",
       "validation  3172  2683  2893\n",
       "test        3180  2573  3071"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Recorre cada partición del dataset (train, test, etc.) y cuenta cuántas veces \n",
    "aparece cada tipo de entidad (como LOC, ORG, PER) al detectar las etiquetas que empiezan por B- (inicio de entidad).\n",
    "\"\"\"\n",
    "from collections import Counter\n",
    "split2freqs = defaultdict(Counter)\n",
    "for split, dataset in panx_de.items():\n",
    "    for row in dataset[\"ner_tags_str\"]:\n",
    "        for tag in row:\n",
    "            if tag.startswith(\"B\"):\n",
    "                tag_type = tag.split(\"-\")[1]\n",
    "                split2freqs[split][tag_type] += 1\n",
    "pd.DataFrame.from_dict(split2freqs, orient=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff93499",
   "metadata": {},
   "source": [
    "Esto se ve bien: las distribuciones de las frecuencias de PER, LOC y ORG son aproximadamente las mismas para cada división, por lo que los conjuntos de validación y prueba deberían proporcionar una buena medida de la capacidad de generalización de nuestro etiquetador NER. A continuación, veamos algunos transformadores multilingües populares y cómo se pueden adaptar para abordar nuestra tarea de NER."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bcaa45",
   "metadata": {},
   "source": [
    "# Multilingual Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d30e2df",
   "metadata": {},
   "source": [
    "Los transformadores multilingües implican arquitecturas y procedimientos de entrenamiento similares a los de sus contrapartes monolingües, excepto que el corpus utilizado para el preentrenamiento consiste en documentos en muchos idiomas. Una característica notable de este enfoque es que, a pesar de no recibir información explícita para diferenciar entre los idiomas, las representaciones lingüísticas resultantes son capaces de generalizar bien entre idiomas para una variedad de tareas posteriores. En algunos casos, esta capacidad de realizar transferencia entre idiomas puede producir resultados que son competitivos con los de los modelos monolingües, lo que elude la necesidad de entrenar un modelo por cada idioma!\n",
    "\n",
    "Para medir el progreso de la transferencia multilingüe para el reconocimiento de entidades nombradas (NER), los conjuntos de datos CoNLL-2002 y CoNLL-2003 se utilizan a menudo como referencia para inglés, neerlandés, español y alemán. Esta referencia consiste en artículos de noticias anotados con las mismas categorías LOC, PER y ORG que PAN-X, pero contiene una etiqueta adicional MISC para entidades diversas que no pertenecen a los tres grupos anteriores. Los modelos de transformadores multilingües suelen ser evaluados de tres maneras diferentes:\n",
    "\n",
    "- `en`: Ajustar finamente los datos de entrenamiento en inglés y luego evaluar en el conjunto de pruebas de cada idioma.\n",
    "- `each`: Ajustar y evaluar en datos de prueba monolingües para medir el rendimiento por idioma.\n",
    "- `all`: Ajustar finamente en todos los datos de entrenamiento para evaluar en cada conjunto de pruebas de cada idioma.\n",
    "\n",
    "Adoptaremos una estrategia de evaluación similar para nuestra tarea de NER, pero primero necesitamos seleccionar un modelo para evaluar. Uno de los primeros transformadores multilingües fue mBERT, que utiliza la misma arquitectura y objetivo de preentrenamiento que BERT, pero añade artículos de Wikipedia de muchos idiomas al corpus de preentrenamiento. Desde entonces, mBERT ha sido superado por XLM-RoBERTa (o XLM-R para abreviar), así que ese es el modelo que consideraremos en este capítulo.\n",
    "\n",
    "Como vimos en el Capítulo 3, XLM-R utiliza solo MLM como un objetivo de preentrenamiento para 100 idiomas, pero se distingue por el enorme tamaño de su corpus de preentrenamiento en comparación con sus precursores: volcado de Wikipedia para cada idioma y 2.5 terabytes de datos de Common Crawl de la web. Este corpus es varios órdenes de magnitud más grande que los utilizados en modelos anteriores y proporciona un impulso significativo en la señal para idiomas de bajos recursos como el birmano y el swahili, donde solo existe un pequeño número de artículos de Wikipedia.\n",
    "\n",
    "La parte del nombre del modelo denominada RoBERTa se refiere al hecho de que el enfoque de preentrenamiento es el mismo que el de los modelos monolingües de RoBERTa. Los desarrolladores de RoBERTa mejoraron varios aspectos de BERT, en particular eliminando por completo la tarea de predicción de la siguiente oración. XLM-R también elimina las incrustaciones de lenguaje usadas en XLM y utiliza SentencePiece para tokenizar los textos sin procesar directamente. Además de su naturaleza multilingüe, una diferencia notable entre XLM-R y RoBERTa es el tamaño de sus respectivos vocabularios: 250,000 tokens frente a 55,000!\n",
    "\n",
    "XLM-R es una excelente opción para tareas de NLU multilingües. En la próxima sección, exploraremos cómo puede tokenizar de manera eficiente en varios idiomas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7aece1",
   "metadata": {},
   "source": [
    "# Una mirada mas cercana a la tokenizacion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e88fc25",
   "metadata": {},
   "source": [
    "En lugar de utilizar un tokenizador WordPiece, XLM-R utiliza un tokenizador llamado SentencePiece que se entrena en el texto en bruto de los cien idiomas. Para entender cómo se compara SentencePiece con WordPiece, carguemos los tokenizadores de BERT y XLM-R de la manera habitual con Transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "621f173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "bert_model_name = \"bert-base-cased\"\n",
    "xlmr_model_name = \"xlm-roberta-base\"\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9654287b",
   "metadata": {},
   "source": [
    "Al codificar una pequeña secuencia de texto, también podemos recuperar los tokens especiales que cada modelo utilizó durante el preentrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d99e18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Jack Sparrow loves New York!\"\n",
    "bert_tokens = bert_tokenizer(text).tokens()\n",
    "xlmr_tokens = xlmr_tokenizer(text).tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc5a640",
   "metadata": {},
   "source": [
    "## The Tokenizer Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9841cb82",
   "metadata": {},
   "source": [
    "Hasta ahora, hemos tratado la tokenización como una sola operación que transforma cadenas en enteros que podemos pasar a través del modelo. Esto no es del todo exacto, y si miramos más de cerca, podemos ver que en realidad es un pipeline de procesamiento completo que generalmente consiste en cuatro pasos:\n",
    "1. `Normalization`: Este paso corresponde al conjunto de operaciones que aplicas a una cadena de texto sin procesar para hacerla \"más limpia\". Las operaciones comunes incluyen eliminar espacios en blanco y quitar caracteres acentuados. La normalización de Unicode es otra operación de normalización común aplicada por muchos tokenizadores para abordar el hecho de que a menudo existen varias maneras de escribir el mismo carácter. Esto puede hacer que dos versiones de la \"misma\" cadena (es decir, con la misma secuencia de caracteres abstractos) parezcan diferentes; los esquemas de normalización de Unicode como NFC, NFD, NFKC y NFKD reemplazan las diversas maneras de escribir el mismo carácter con formas estándar. Otro ejemplo de normalización es convertir a minúsculas. Si se espera que el modelo acepte y utilice solo caracteres en minúscula, esta técnica se puede usar para reducir el tamaño del vocabulario que requiere. Después de la normalización, nuestra cadena de ejemplo se vería como \"¡jack sparrow ama nueva york!\".\n",
    "2. `Pretokenization`: Este paso divide un texto en objetos más pequeños que proporcionan un límite superior a lo que serán tus tokens al final del entrenamiento. Una buena manera de pensar en esto es que el pretokenizador dividirá tu texto en \"palabras\", y tus tokens finales serán partes de esas palabras. Para los idiomas que permiten esto (inglés, alemán y muchos idiomas indoeuropeos), las cadenas generalmente pueden dividirse en palabras mediante espacios en blanco y puntuación. Por ejemplo, este paso puede transformar nuestro [\"jack\", \"sparrow\", \"loves\", \"new\", \"york\", \"!\"];.Estas palabras son más simples de dividir en subpalabras con algoritmos de Byte-Pair Encoding (BPE) o Unigram en el siguiente paso de la tubería. Sin embargo, dividir en \"palabras\" no siempre es una operación trivial y determinista, o incluso una operación que tenga sentido. Por ejemplo, en idiomas como el chino, japonés o coreano, agrupar símbolos en unidades semánticas como las palabras indoeuropeas puede ser una operación no determinista con varios grupos igualmente válidos. En este caso, podría ser mejor no pretokenizar el texto y, en su lugar, utilizar una biblioteca específica para el idioma para la pretokenización.\n",
    "3. `Tokenizer model`: Una vez que los textos de entrada están normalizados y pretokens, el tokenizador aplica un modelo de división de subpalabras sobre las palabras. Esta es la parte del proceso que necesita ser entrenada en su corpus (o que ha sido entrenada si está utilizando un tokenizador preentrenado). El papel del modelo es dividir las palabras en subpalabras para reducir el tamaño del vocabulario y tratar de disminuir el número de tokens fuera de vocabulario. Existen varios algoritmos de tokenización de subpalabras, incluyendo BPE, Unigram y WordPiece. Por ejemplo, nuestro ejemplo en funcionamiento podría verse como [jack, spa, rrow, loves, new, york, !] después de aplicar el modelo tokenizador. Tenga en cuenta que en este punto ya no tenemos una lista de cadenas sino una lista de enteros (ID de entrada); para mantener el ejemplo ilustrativo, hemos conservado las palabras pero hemos quitado las comillas para indicar la transformación.\n",
    "4. `Postprocessing`:Este es el último paso del proceso de tokenización, en el cual se pueden aplicar algunas transformaciones adicionales a la lista de tokens; por ejemplo, agregar tokens especiales al principio o al final de la secuencia de entrada de índices de tokens. Por ejemplo, un tokenizador estilo BERT agregaría tokens de clasificación y separación: [CLS, jack, spa, rrow, ama, nueva, york, !, SEP]. Esta secuencia (recuerda que será una secuencia de números enteros, no los tokens que ves aquí) puede ser alimentada al modelo.\n",
    "\n",
    "Volviendo a nuestra comparación de XLM-R y BERT, ahora entendemos que SentencePiece agrega \"s y \\s\" en lugar de [CLS] y [SEP] en el paso de posprocesamiento (como convención, continuaremos usando [CLS] y [SEP] en las ilustraciones gráficas). Volvamos al tokenizador SentencePiece para ver qué lo hace especial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7d790d",
   "metadata": {},
   "source": [
    "## El tokenizador SentencePiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d96a27",
   "metadata": {},
   "source": [
    "El tokenizador SentencePiece se basa en un tipo de segmentación de subpalabras llamada Unigram y codifica cada texto de entrada como una secuencia de caracteres Unicode. Esta última característica es especialmente útil para corpora multilingües, ya que permite que SentencePiece sea agnóstico sobre acentos, puntuación y el hecho de que muchos idiomas, como el japonés, no tienen caracteres de espacio en blanco. Otra característica especial de SentencePiece es que el espacio en blanco se asigna al símbolo Unicode U+2581, o al carácter , también llamado el carácter de bloque inferior.\n",
    "\n",
    "Esto permite que SentencePiece des-tokenice una secuencia sin ambigüedades y sin depender de pre-tokenizadores específicos del idioma. En nuestro ejemplo de la sección anterior, por ejemplo, podemos ver que WordPiece ha perdido la información de que no hay espacio en blanco entre \"York\" y \"!\". Por el contrario, SentencePiece preserva el espacio en blanco en el texto tokenizado, por lo que podemos convertir de nuevo al texto sin procesar sin ambigüedad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fe60593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Jack Sparrow loves New York!</s>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(xlmr_tokens).replace(u\"\\u2581\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15cb51c",
   "metadata": {},
   "source": [
    "Ahora que entendemos cómo funciona SentencePiece, veamos cómo podemos codificar nuestro ejemplo simple en una forma adecuada para NER. Lo primero que debe hacer es cargar el modelo previamente entrenado con un cabezal de clasificación de tokens. ¡Pero en lugar de cargar esta cabeza directamente desde los Transformadores, la construiremos nosotros mismos! Al profundizar en la API de Transformers, podemos hacer esto con solo unos pocos pasos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab86412",
   "metadata": {},
   "source": [
    "# Transformadores para Reconocimiento de Entidades Nombradas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ffb9e7",
   "metadata": {},
   "source": [
    "En el capítulo 2, vimos que para la clasificación de texto BERT usa el token especial [CLS] para representar una secuencia completa de texto. Esta representación se alimenta luego a través de una capa densa o completamente conectada para generar la distribución de todos los valores de etiqueta discretos, como se muestra en Figura 4-2.\n",
    "\n",
    "![TokenizadoBert](images/tokenizadorBert.png)\n",
    "\n",
    "Figura 4-2. *Ajuste fino de un transformador basado en codificador para la clasificación de secuencias*\n",
    "\n",
    "BERT y otros transformadores de solo codificador adoptan un enfoque similar para NER, excepto que la representación de cada token de entrada individual se alimenta a la misma capa completamente conectada para generar la entidad del token. Por esta razón, NER a menudo se enmarca como una tarea de clasificación de tokens. El proceso se parece al diagrama de la Figura 4-3.\n",
    "\n",
    "![Ner](images/NEr.png)\n",
    "\n",
    "Figura 4-3. *Ajuste fino de un transformador basado en codificador para el reconocimiento de entidades nombradas*\n",
    "\n",
    "Hasta ahora, todo bien, pero `¿cómo deberíamos manejar las subpalabras en una tarea de clasificación de tokens?` Por ejemplo, el primer nombre \"Christa\" en la Figura 4-3 está tokenizado en las subpalabras \" Chr \" y \"##ist\", entonces, ¿a cuál(es) se le debe asignar la etiqueta B-PER?\n",
    "\n",
    "En el artículo de BERT, los autores asignaron esta etiqueta a la primera subpalabra (\"Chr\" en nuestro ejemplo) e ignoraron la siguiente subpalabra (\"##ist\"). Esta es la convención que adoptaremos aquí, e indicaremos las subpalabras ignoradas con IGN. Posteriormente, podemos propagar fácilmente la etiqueta predicha de la primera subpalabra a las subpalabras posteriores en el paso de posprocesamiento. También podríamos haber optado por incluir la representación de la subpalabra \"# # ist \" asignándole una copia de la etiqueta B-LOC, pero esto viola la Formato IOB2.\n",
    "\n",
    "Afortunadamente, todos los aspectos de la arquitectura que hemos visto en BERT se trasladan a XLM-R ya que su arquitectura se basa en RoBERTa, que es idéntica a ¡BERT! A continuación veremos cómo Transformers admite muchas otras tareas con modificaciones menores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08db9d2f",
   "metadata": {},
   "source": [
    "# The Anatomy of the Transformers Model Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f672a451",
   "metadata": {},
   "source": [
    "Transformers se organiza en torno a clases dedicadas para cada arquitectura y tarea. Las clases de modelo asociadas con diferentes tareas se nombran de acuerdo con una convención<Model Name>For < Task>, o AutoModelFor < Task> cuando se usa el Clases de automodelo.\n",
    "\n",
    "Sin embargo, este enfoque tiene sus limitaciones, y para motivar a profundizar en la API de Transformers, considere el siguiente escenario. Supongamos que tiene una gran idea para resolver un problema de PNL que ha estado en su mente durante mucho tiempo con un modelo de transformador. Entonces, programa una reunión con su jefe y, con una presentación de PowerPoint ingeniosamente elaborada, presenta que podría aumentar los ingresos de su departamento si finalmente puedes resolver el problema. Impresionado con su colorida presentación y charla sobre ganancias, su jefe acepta generosamente darle una semana para crear una prueba de concepto.\n",
    "\n",
    "Contento con el resultado, empiezas a trabajar de inmediato. Enciendes tu GPU y abres una computadora portátil. Ejecutas desde transformers import Bert ForTaskXY (ten en cuenta que TaskXY es la tarea imaginaria que te gustaría resolver) y el color escapa de tu rostro como el temido color rojo llena tu pantalla: ImportError: no se puede importar el nombre BertForTaskXY. ¡Oh, no, no hay un modelo BERT para su caso de uso! ¿Cómo puede completar el proyecto en una semana si tiene que implementar todo el modelo usted mismo?! ¿Por dónde deberías empezar?\n",
    "\n",
    "¡Que no cunda el pánico! Transformers está diseñado para permitirle ampliar fácilmente los modelos existentes para su caso de uso específico. Puede cargar los pesos desde modelos previamente entrenados y tiene acceso a funciones auxiliares específicas de la tarea. Esto le permite crear modelos personalizados para objetivos específicos con muy poca sobrecarga. En esta sección, veremos cómo podemos implementar nuestro propio modelo personalizado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d2bc03",
   "metadata": {},
   "source": [
    "## Bodies and Heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36557cf9",
   "metadata": {},
   "source": [
    "El concepto principal que hace que los Transformers sean tan versátiles es la división de la arquitectura en un cuerpo y una cabeza (como vimos en el Capítulo 1). Ya hemos visto que cuando pasamos de la tarea de preentrenamiento a la tarea específica, necesitamos reemplazar la última capa del modelo por una que sea adecuada para la tarea. Esta última capa se llama cabeza del modelo; es la parte que es específica de la tarea.\n",
    "\n",
    "El resto del modelo se llama el cuerpo; incluye las embeddings de token y las capas de transformador que son independientes de la tarea. Este código de Transformadores también es así: la estructura del cuerpo de un modelo se refleja en su implementación en una clase como BertModel o GPT2Model que devuelve los estados ocultos de la última capa. Modelos específicos para tareas como BertForMaskedLM o BertForSequence Classification utilizan el modelo base y añaden la cabeza necesaria sobre los estados ocultos, como se muestra en la Figura 4-4.\n",
    "\n",
    "![Bertmodel](images/Bertmodel.png)\n",
    "\n",
    "Fig 4-4. *La clase BertModel solo contiene el cuerpo del modelo, mientras que las clases Bert For<Task> combinan el cuerpo con una cabeza dedicada para una tarea específica.*\n",
    "\n",
    "Como veremos a continuación, esta separación de cuerpos y cabezas nos permite construir una cabeza personalizada para cualquier tarea y simplemente montarla sobre un modelo preentrenado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58baa47e",
   "metadata": {},
   "source": [
    "## Creando un modelo personalizado para clasificacion de tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98dc70a",
   "metadata": {},
   "source": [
    "Vamos a realizar el ejercicio de construir una cabecera de clasificación de tokens personalizada para XLM-R. Dado que XLM-R utiliza la misma arquitectura de modelo que RoBERTa, utilizaremos RoBERTa como modelo base, pero augmentado con configuraciones específicas de XLM-R. Tenga en cuenta que este es un ejercicio educativo para mostrarle cómo construir un modelo personalizado para su propia tarea. Para la clasificación de tokens, ya existe una clase XLMRobertaForTokenClassification que puede importar de Transformers. Si lo desea, puede saltar a la siguiente sección y simplemente usar esa.\n",
    "\n",
    "Para comenzar, necesitamos una estructura de datos que represente nuestro etiquetador XLM-R NER. Como primera suposición, necesitaremos un objeto de configuración para inicializar el modelo y una función forward() para generar los resultados. Sigamos adelante y construyamos nuestra clase XLM-R para la clasificación de tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cc7212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import XLMRobertaConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel\n",
    "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n",
    "\n",
    "class XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n",
    "    config_class = XLMRobertaConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        # Load model body\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "        # Set up token classification head\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        # Load and initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,labels=None, **kwargs):\n",
    "        # Use model body to get encoder representations\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask,\n",
    "                               token_type_ids=token_type_ids, **kwargs)\n",
    "        # Apply classifier to encoder representation\n",
    "        sequence_output = self.dropout(outputs[0])\n",
    "        logits = self.classifier(sequence_output)\n",
    "        # Calculate losses\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        # Return model output object\n",
    "        return TokenClassifierOutput(loss=loss, \n",
    "                                     logits=logits,hidden_states=outputs.hidden_states,\n",
    "                                     attentions=outputs.attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2726eef6",
   "metadata": {},
   "source": [
    "La clase config_class asegura que se utilicen los ajustes estándar de XLM-R al inicializar un nuevo modelo. Si deseas cambiar los parámetros predeterminados, puedes hacerlo sobrescribiendo la configuración predeterminada. Con el método super() llamamos a la función de inicialización de la clase RobertaPreTrainedModel. Esta clase abstracta maneja la inicialización o la carga de pesos preentrenados. Luego cargamos el cuerpo de nuestro modelo, que es RobertaModel, y lo extendemos con nuestra propia cabeza de clasificación que consiste en una capa de abandono y una capa de alimentación hacia adelante estándar.\n",
    "\n",
    "Tenga en cuenta que establecemos add_pooling_layer=False para asegurarnos de que se devuelvan todos los estados ocultos y no solo el asociado con el token [CLS]. Finalmente, inicializamos todos los pesos llamando al método init_weights() que heredamos de RobertaPreTrainedModel, que cargará los pesos preentrenados para el cuerpo del modelo y inicializará aleatoriamente los pesos de nuestra cabeza de clasificación de tokens.\n",
    "\n",
    "Lo único que queda por hacer es definir lo que el modelo debe hacer en una pasada hacia adelante con un método forward(). Durante la pasada hacia adelante, los datos se alimentan primero a través del cuerpo del modelo. Hay una serie de variables de entrada, pero las únicas que necesitamos por ahora son input_ids y attention_mask. El estado oculto, que es parte de la salida del cuerpo del modelo, se alimenta luego a través de las capas de abandono y clasificación. Si también proporcionamos etiquetas en la pasada hacia adelante, podemos calcular directamente la pérdida. Si hay una máscara de atención, necesitamos hacer un poco más de trabajo para asegurarnos de que solo calculamos la pérdida de los tokens no enmascarados. Finalmente, envolvemos todas las salidas en un objeto TokenClassifierOutput que nos permite acceder a los elementos en una tupla nombrada familiar de capítulos anteriores.\n",
    "\n",
    "Al implementar solo dos funciones de una clase simple, podemos construir nuestro propio modelo transformador personalizado. Y dado que heredamos de un PreTrainedModel, ¡obtendemos instantáneamente utilidades de Transformer, como from_pretrained()! Vamos a acceder a todo lo útil y veamos cómo podemos cargar pesos preentrenados en nuestro modelo personalizado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc53f56",
   "metadata": {},
   "source": [
    "## Cargando un modelo personalizado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2406700",
   "metadata": {},
   "source": [
    "Ahora estamos listos para cargar nuestro modelo de clasificación de tokens. Necesitaremos proporcionar información adicional más allá del nombre del modelo, incluyendo las etiquetas que utilizaremos para etiquetar cada entidad y el mapeo de cada etiqueta a un ID y viceversa. Toda esta información se puede derivar de nuestra variable de etiquetas, que como objeto ClassLabel tiene un atributo names que podemos usar para obtener el mapeo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e681f252",
   "metadata": {},
   "outputs": [],
   "source": [
    "index2tag = {idx: tag for idx, tag in enumerate(tags.names)}\n",
    "tag2index = {tag: idx for idx, tag in enumerate(tags.names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b58f244c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC'}\n",
      "{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6}\n"
     ]
    }
   ],
   "source": [
    "print(index2tag)\n",
    "print(tag2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e308145f",
   "metadata": {},
   "source": [
    "Almacenaremos estas asignaciones y el atributo tags.num_classes en el objeto AutoConfig que encontramos en el Capítulo 3. Pasar argumentos de palabra clave al método from_pretrained() reemplaza los valores predeterminados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "729be39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "xlmr_config = AutoConfig.from_pretrained(xlmr_model_name,\n",
    "                                         num_labels=tags.num_classes,\n",
    "                                         id2label=index2tag, \n",
    "                                         label2id=tag2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f52887",
   "metadata": {},
   "source": [
    "La clase AutoConfig contiene el plano de la arquitectura de un modelo. Cuando cargamos un modelo con AutoModel.from_pretrained(model_ckpt), el archivo de configuración asociado con ese modelo se descarga automáticamente. Sin embargo, si queremos modificar algo como el número de clases o los nombres de las etiquetas, entonces podemos cargar la configuración primero con los parámetros que nos gustaría personalizar.\n",
    "\n",
    "Ahora, podemos cargar los pesos del modelo como de costumbre con la función from_pretrained() con el argumento de configuración adicional. Tenga en cuenta que no implementamos la carga de pesos preentrenados en nuestra clase de modelo personalizada; obtenemos esto de forma gratuita al heredar de RobertaPreTrainedModel:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05b2d551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "xlmr_model = (XLMRobertaForTokenClassification.from_pretrained(xlmr_model_name, config=xlmr_config).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d0b95a",
   "metadata": {},
   "source": [
    "Como una verificación rápida de que hemos inicializado correctamente el tokenizador y el modelo, vamos a probar las predicciones en nuestra pequeña secuencia de entidades conocidas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e226cd8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁Jack</td>\n",
       "      <td>▁Spar</td>\n",
       "      <td>row</td>\n",
       "      <td>▁love</td>\n",
       "      <td>s</td>\n",
       "      <td>▁New</td>\n",
       "      <td>▁York</td>\n",
       "      <td>!</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Input IDs</th>\n",
       "      <td>0</td>\n",
       "      <td>21763</td>\n",
       "      <td>37456</td>\n",
       "      <td>15555</td>\n",
       "      <td>5161</td>\n",
       "      <td>7</td>\n",
       "      <td>2356</td>\n",
       "      <td>5753</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0      1      2      3      4  5     6      7   8     9\n",
       "Tokens     <s>  ▁Jack  ▁Spar    row  ▁love  s  ▁New  ▁York   !  </s>\n",
       "Input IDs    0  21763  37456  15555   5161  7  2356   5753  38     2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = xlmr_tokenizer.encode(text, return_tensors=\"pt\")\n",
    "pd.DataFrame([xlmr_tokens, input_ids[0].numpy()], index=[\"Tokens\", \"Input IDs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f55dbde",
   "metadata": {},
   "source": [
    "Como puedes ver aquí, los tokens de inicio <s> y fin </s> se les asignan los ID 0 y 2, respectivamente.\n",
    "Finalmente, necesitamos pasar las entradas al modelo y extraer las predicciones tomando el argmax para obtener la clase más probable por token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72cde587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in sequence: 10\n",
      "Shape of outputs: torch.Size([1, 10, 7])\n"
     ]
    }
   ],
   "source": [
    "outputs = xlmr_model(input_ids.to(device)).logits\n",
    "predictions = torch.argmax(outputs, dim=-1)\n",
    "print(f\"Number of tokens in sequence: {len(xlmr_tokens)}\")\n",
    "print(f\"Shape of outputs: {outputs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442a6748",
   "metadata": {},
   "source": [
    "Aquí vemos que los logits tienen la forma [tamaño_batch, num_tokens, num_tags], con cada token recibiendo un logit entre las siete posibles etiquetas de NER. Al enumerar la secuencia, podemos ver rápidamente lo que el modelo preentrenado predice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebee645e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁Jack</td>\n",
       "      <td>▁Spar</td>\n",
       "      <td>row</td>\n",
       "      <td>▁love</td>\n",
       "      <td>s</td>\n",
       "      <td>▁New</td>\n",
       "      <td>▁York</td>\n",
       "      <td>!</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0      1      2      3      4      5      6      7      8      9\n",
       "Tokens    <s>  ▁Jack  ▁Spar    row  ▁love      s   ▁New  ▁York      !   </s>\n",
       "Tags    B-LOC  B-LOC  B-LOC  B-LOC  B-LOC  B-LOC  B-LOC  B-LOC  B-LOC  B-LOC"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n",
    "pd.DataFrame([xlmr_tokens, preds], index=[\"Tokens\", \"Tags\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f877686",
   "metadata": {},
   "source": [
    "Como era de esperar, nuestra capa de clasificación de tokens con pesos aleatorios deja mucho que desear; ¡ajustemos un poco con datos etiquetados para mejorarla! Antes de hacerlo, envolvamos los pasos anteriores en una función auxiliar para su uso posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9374b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_text(text, tags, model, tokenizer):\n",
    "    # Get tokens with special characters\n",
    "    tokens = tokenizer(text).tokens()\n",
    "    # Encode the sequence into IDs\n",
    "    input_ids = xlmr_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    # Get predictions as distribution over 7 possible classes\n",
    "    outputs = model(input_ids)[0]\n",
    "    # Take argmax to get most likely class per token\n",
    "    predictions = torch.argmax(outputs, dim=2)\n",
    "    # Convert to DataFrame\n",
    "    preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n",
    "    return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a65758b",
   "metadata": {},
   "source": [
    "Antes de que podamos entrenar el modelo, también necesitamos tokenizar las entradas y preparar las etiquetas. Haremos eso a continuación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024200cb",
   "metadata": {},
   "source": [
    "# Tokenizando textos para el NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f6bed6",
   "metadata": {},
   "source": [
    "Ahora que hemos establecido que el tokenizador y el modelo pueden codificar un solo ejemplo, nuestro siguiente paso es tokenizar todo el conjunto de datos para poder pasarlo al modelo XLM-R para su ajuste fino. Como vimos en el Capítulo 2, Datasets proporciona una manera rápida de tokenizar un objeto Dataset con la operación map(). Para lograr esto, recordemos que primero necesitamos definir una función con la firma mínima:\n",
    "\n",
    "*function(examples: Dict[str, List]) -> Dict[str, List]*\n",
    "\n",
    "donde examples es equivalente a una porción de un Conjunto de Datos, por ejemplo, panx_de['train'][:10]. Dado que el tokenizador XLM-R devuelve los ID de entrada para las entradas del modelo, solo necesitamos aumentar esta información con la máscara de atención y los ID de etiquetas que codifican la información sobre qué token está asociado con cada etiqueta NER.\n",
    "\n",
    "Siguiendo el enfoque adoptado en cómo funciona esto con nuestro único ejemplo en alemán, recopilando primero las palabras y etiquetas como listas ordinarias:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ab4d89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words, labels = de_example[\"tokens\"], de_example[\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fad61e1",
   "metadata": {},
   "source": [
    "A continuación, tokenizamos cada palabra y usamos el argumento bis_split_into_words para decirle al tokenizador que nuestra secuencia de entrada ya se ha dividido en palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da156362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁2.000</td>\n",
       "      <td>▁Einwohner</td>\n",
       "      <td>n</td>\n",
       "      <td>▁an</td>\n",
       "      <td>▁der</td>\n",
       "      <td>▁Dan</td>\n",
       "      <td>zi</td>\n",
       "      <td>ger</td>\n",
       "      <td>▁Buch</td>\n",
       "      <td>...</td>\n",
       "      <td>▁Wo</td>\n",
       "      <td>i</td>\n",
       "      <td>wod</td>\n",
       "      <td>schaft</td>\n",
       "      <td>▁Po</td>\n",
       "      <td>mmer</td>\n",
       "      <td>n</td>\n",
       "      <td>▁</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1           2  3    4     5     6   7    8      9   ...   15  \\\n",
       "Tokens  <s>  ▁2.000  ▁Einwohner  n  ▁an  ▁der  ▁Dan  zi  ger  ▁Buch  ...  ▁Wo   \n",
       "\n",
       "       16   17      18   19    20 21 22 23    24  \n",
       "Tokens  i  wod  schaft  ▁Po  mmer  n  ▁  .  </s>  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_input = xlmr_tokenizer(de_example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = xlmr_tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "pd.DataFrame([tokens], index=[\"Tokens\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc92524",
   "metadata": {},
   "source": [
    "En este ejemplo podemos ver que el tokenizador ha dividido “Einwohnern” en dos subpalabras, “Einwohner” y “n”. Dado que seguimos la convención de que solo “Einwohner” debe asociarse con la etiqueta B-LOC, necesitamos una forma de enmascarar las representaciones de subpalabras después de la primera subpalabra. Afortunadamente, tokenized_input es una clase que contiene una función word_ids() que puede ayudarnos a lograr esto:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73324ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁2.000</td>\n",
       "      <td>▁Einwohner</td>\n",
       "      <td>n</td>\n",
       "      <td>▁an</td>\n",
       "      <td>▁der</td>\n",
       "      <td>▁Dan</td>\n",
       "      <td>zi</td>\n",
       "      <td>ger</td>\n",
       "      <td>▁Buch</td>\n",
       "      <td>...</td>\n",
       "      <td>▁Wo</td>\n",
       "      <td>i</td>\n",
       "      <td>wod</td>\n",
       "      <td>schaft</td>\n",
       "      <td>▁Po</td>\n",
       "      <td>mmer</td>\n",
       "      <td>n</td>\n",
       "      <td>▁</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word IDs</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0       1           2  3    4     5     6   7    8      9   ...  \\\n",
       "Tokens     <s>  ▁2.000  ▁Einwohner  n  ▁an  ▁der  ▁Dan  zi  ger  ▁Buch  ...   \n",
       "Word IDs  None       0           1  1    2     3     4   4    4      5  ...   \n",
       "\n",
       "           15 16   17      18   19    20  21  22  23    24  \n",
       "Tokens    ▁Wo  i  wod  schaft  ▁Po  mmer   n   ▁   .  </s>  \n",
       "Word IDs    9  9    9       9   10    10  10  11  11  None  \n",
       "\n",
       "[2 rows x 25 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids = tokenized_input.word_ids()\n",
    "pd.DataFrame([tokens, word_ids], index=[\"Tokens\", \"Word IDs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e38685",
   "metadata": {},
   "source": [
    "Aquí podemos ver que word_ids ha asignado cada subpalabra al índice correspondiente en la secuencia de palabras, por lo que a la primera subpalabra, \"2.000\", se le asigna el índice 0, mientras que a \"Einwohner\" y \"n\" se les asigna el índice 1 (ya que \"Einwohnern\" es la segunda palabra en palabras). También podemos ver que los tokens especiales como `<s>` y `<\\s>` se asignan a None. Establezcamos –100 como la etiqueta para estas fichas especiales y las subpalabras que deseamos enmascarar durante el entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0af6842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁2.000</td>\n",
       "      <td>▁Einwohner</td>\n",
       "      <td>n</td>\n",
       "      <td>▁an</td>\n",
       "      <td>▁der</td>\n",
       "      <td>▁Dan</td>\n",
       "      <td>zi</td>\n",
       "      <td>ger</td>\n",
       "      <td>▁Buch</td>\n",
       "      <td>...</td>\n",
       "      <td>▁Wo</td>\n",
       "      <td>i</td>\n",
       "      <td>wod</td>\n",
       "      <td>schaft</td>\n",
       "      <td>▁Po</td>\n",
       "      <td>mmer</td>\n",
       "      <td>n</td>\n",
       "      <td>▁</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word IDs</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label IDs</th>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>6</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Labels</th>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>...</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0       1           2     3    4     5      6     7     8   \\\n",
       "Tokens      <s>  ▁2.000  ▁Einwohner     n  ▁an  ▁der   ▁Dan    zi   ger   \n",
       "Word IDs   None       0           1     1    2     3      4     4     4   \n",
       "Label IDs  -100       0           0  -100    0     0      5  -100  -100   \n",
       "Labels      IGN       O           O   IGN    O     O  B-LOC   IGN   IGN   \n",
       "\n",
       "              9   ...     15    16    17      18     19    20    21  22    23  \\\n",
       "Tokens     ▁Buch  ...    ▁Wo     i   wod  schaft    ▁Po  mmer     n   ▁     .   \n",
       "Word IDs       5  ...      9     9     9       9     10    10    10  11    11   \n",
       "Label IDs      6  ...      5  -100  -100    -100      6  -100  -100   0  -100   \n",
       "Labels     I-LOC  ...  B-LOC   IGN   IGN     IGN  I-LOC   IGN   IGN   O   IGN   \n",
       "\n",
       "             24  \n",
       "Tokens     </s>  \n",
       "Word IDs   None  \n",
       "Label IDs  -100  \n",
       "Labels      IGN  \n",
       "\n",
       "[4 rows x 25 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_word_idx = None\n",
    "label_ids = []\n",
    "\n",
    "for word_idx in word_ids:\n",
    "    if word_idx is None or word_idx == previous_word_idx:\n",
    "        label_ids.append(-100)\n",
    "    elif word_idx != previous_word_idx:\n",
    "        label_ids.append(labels[word_idx])\n",
    "    previous_word_idx = word_idx\n",
    "labels = [index2tag[l] if l != -100 else \"IGN\" for l in label_ids]\n",
    "index = [\"Tokens\", \"Word IDs\", \"Label IDs\", \"Labels\"]\n",
    "pd.DataFrame([tokens, word_ids, label_ids, labels], index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c836ff5",
   "metadata": {},
   "source": [
    "`¿Por qué elegimos –100 como el ID para enmascarar las representaciones de subpalabras? La razón es que en PyTorch, la clase de pérdida de entropía cruzada torch.nn.CrossEntropyLoss tiene un atributo llamado ignore_index cuyo valor es –100. Este índice se ignora durante el entrenamiento, por lo que podemos usarlo para ignorar los tokens asociados con subpalabras consecutivas.`\n",
    "\n",
    "\n",
    "¡Y eso es todo! Podemos ver claramente cómo los identificadores de las etiquetas se alinean con los tokens, así que escalemos esto a todo el conjunto de datos definiendo una sola función que envuelva toda la lógica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b04b8645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = xlmr_tokenizer(examples[\"tokens\"], truncation=True,is_split_into_words=True)\n",
    "    labels = []\n",
    "    for idx, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None or word_idx == previous_word_idx:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c919f5a",
   "metadata": {},
   "source": [
    "Ahora tenemos todos los ingredientes que necesitamos para codificar cada división, así que escribamos una función que podamos iterar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3736a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_panx_dataset(corpus):\n",
    "    return corpus.map(tokenize_and_align_labels, batched=True,\n",
    "    remove_columns=['langs', 'ner_tags', 'tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a234abc1",
   "metadata": {},
   "source": [
    "Al aplicar esta función a un objeto DatasetDict, obtenemos un objeto Dataset codificado por división. Usemos esto para codificar nuestro corpus alemán:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "763b98f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "584f5bc3ec45406abaef3c92134e22d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12580 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fb7762e687d49f0bcadc87f98c598e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6290 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b28c159fae4e53a08cb9a8fc678a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6290 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "panx_de_encoded = encode_panx_dataset(panx_ch[\"de\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b250ef6d",
   "metadata": {},
   "source": [
    "Ahora que tenemos un modelo y un conjunto de datos, necesitamos definir una métrica de rendimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86854aef",
   "metadata": {},
   "source": [
    "## Medidas de desempeño"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d872ed16",
   "metadata": {},
   "source": [
    "Evaluar un modelo de NER es similar a evaluar un modelo de clasificación de texto, y es común reportar resultados de precisión, recall y F1-score. La única sutileza es que todas las palabras de una entidad deben ser predichas correctamente para que una predicción se cuente como correcta. Afortunadamente, hay una excelente biblioteca llamada seqeval que está diseñada para este tipo de tareas. Por ejemplo, dado algunos tags de NER de marcador de posición y las predicciones del modelo, podemos calcular las métricas a través de la función classification_report() de seqeval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee76b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
