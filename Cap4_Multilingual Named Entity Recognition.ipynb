{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f44346b8",
   "metadata": {},
   "source": [
    "**Tabla de contenido**\n",
    "\n",
    "- [The Dataset](#The-Dataset)\n",
    "- [Multilingual Transformers](#Multilingual-Transformers)\n",
    "- [Una mirada más cercana a la tokenización](#Una-mirada-mas-cercana-a-la-tokenizacion)\n",
    "     - [The Tokenizer Pipeline](#The-Tokenizer-Pipeline)\n",
    "     - [El tokenizador SentencePiece](#El-tokenizador-SentencePiece)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926ce0da",
   "metadata": {},
   "source": [
    "Hasta ahora en este libro hemos aplicado transformadores para resolver tareas de NLP en corpora en inglés, pero `¿qué haces cuando tus documentos están escritos en griego, swahili o klingon?` Un enfoque es buscar en el Hugging Face Hub un modelo de lenguaje preentrenado adecuado y ajustarlo para la tarea en cuestión. Sin embargo, estos modelos preentrenados tienden a existir solo para lenguajes de \"alto recurso\" como alemán, ruso o mandarín, donde hay mucho texto web disponible para el preentrenamiento. Otro desafío común surge cuando tu corpus es multilingüe: mantener múltiples modelos monolingües en producción no será divertido ni para ti ni para tu equipo de ingeniería.\n",
    "\n",
    "Afortunadamente, existe una clase de transformadores multilingües que vienen al rescate. Al igual que BERT, estos modelos utilizan el modelado de lenguaje enmascarado como objetivo de preentrenamiento, pero se entrenan conjuntamente en textos en más de cien idiomas. Al preentrenarse en grandes corpus a través de muchos idiomas, estos transformadores multilingües permiten la transferencia cruzada de cero disparos. Esto significa que un modelo que se ajusta finamente a un idioma puede aplicarse a otros sin ningún entrenamiento adicional. ¡Esto también hace que estos modelos sean muy adecuados para el 'cambio de código', donde un hablante alterna entre dos o más idiomas o dialectos en el contexto de una sola conversación!\n",
    "\n",
    "En este capítulo exploraremos cómo un único modelo de transformador llamado XLM-RoBERTa (introducido en el Capítulo 3) puede ser ajustado para realizar el reconocimiento de entidades nombradas (NER) en varios idiomas. Como vimos en el Capítulo 1, el NER es una tarea común de procesamiento del lenguaje natural que identifica entidades como personas, organizaciones o ubicaciones en el texto. Estas entidades pueden ser utilizadas para diversas aplicaciones, como obtener información de documentos de empresas, mejorar la calidad de los motores de búsqueda, o simplemente construir una base de datos estructurada a partir de un corpus.\n",
    "\n",
    "Para este capítulo, supongamos que queremos realizar NER para un cliente con sede en Suiza, donde hay cuatro idiomas nacionales (siendo el inglés a menudo un puente entre ellos). Empecemos obteniendo un corpus multilingüe adecuado para este problema.\n",
    "\n",
    "Nota: `la Zero-shot transfer`  o el `zero-shot learning` generalmente se refiere a la tarea de entrenar un modelo en un conjunto de etiquetas y luego evaluarlo en un conjunto diferente de etiquetas. En el contexto de los transformadores, el zero-shot learning también puede referirse a situaciones en las que un modelo de lenguaje como GPT-3 es evaluado en una tarea posterior en la que ni siquiera se ha ajustado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157395a3",
   "metadata": {},
   "source": [
    "# The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08394d8f",
   "metadata": {},
   "source": [
    "En este capítulo, utilizaremos un subconjunto del benchmark de Evaluación de Transferencia Multilingüe de Encoders (XTREME) llamado WikiANN o PAN-X. Este conjunto de datos consiste en artículos de Wikipedia en muchos idiomas, incluidos los cuatro idiomas más hablados en Suiza: alemán (62.9%), francés (22.9%), italiano (8.4%) e inglés (5.9%).\n",
    "\n",
    "Cada artículo está anotado con etiquetas LOC (ubicación), PER (persona) y ORG (organización) en el formato \"dentro-fuera-comienzo\" (IOB2). En este formato, un prefijo B indica el comienzo de una entidad, y los tokens consecutivos que pertenecen a la misma entidad reciben un prefijo I-. Una etiqueta O indica que el token no pertenece a ninguna entidad. Por ejemplo, la siguiente oración:\n",
    "\n",
    " `Jeff Dean is a computer scientist at Google in California` \n",
    " \n",
    " se etiquetaría en formato IOB2 como se muestra en la siguiente Tabla.\n",
    "\n",
    "![Tabla](images/table4.png)\n",
    "\n",
    "Para cargar uno de los subconjuntos de PAN-X en XTREME, necesitaremos saber qué configuración de conjunto de datos pasar a la función load_dataset(). Siempre que trabajes con un conjunto de datos que tiene múltiples dominios, puedes usar la función get_dataset_config_names() para averiguar qué subconjuntos están disponibles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1476fdd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9a0e44d7654127b7aa1a12cb361e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XTREME has 183 configurations\n"
     ]
    }
   ],
   "source": [
    "from datasets import get_dataset_config_names\n",
    "xtreme_subsets = get_dataset_config_names(\"xtreme\")\n",
    "print(f\"XTREME has {len(xtreme_subsets)} configurations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12959e0",
   "metadata": {},
   "source": [
    "Vaya, ¡eso es un montón de configuraciones! Vamos a reducir la búsqueda buscando solo las configuraciones que comienzan con \"PAN\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccc0679f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PAN-X.af', 'PAN-X.ar', 'PAN-X.bg']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panx_subsets = [s for s in xtreme_subsets if s.startswith(\"PAN\")]\n",
    "panx_subsets[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b406cba5",
   "metadata": {},
   "source": [
    "De acuerdo, parece que hemos identificado la sintaxis de los subconjuntos de PAN-X: cada uno tiene un sufijo de dos letras que parece ser un código de idioma ISO 639-1. Esto significa que para cargar el corpus en alemán, pasamos el código al argumento name de load_dataset() de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c063ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6b5faeeba94760bc0e81ed05d9a148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PAN-X.de/train-00000-of-00001.parquet:   0%|          | 0.00/1.18M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b122b5a10b5e4742867693159d93b3c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PAN-X.de/validation-00000-of-00001.parqu(…):   0%|          | 0.00/590k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e1c0d5a0d2433c83baa6c6a74ac04d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PAN-X.de/test-00000-of-00001.parquet:   0%|          | 0.00/588k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f811e69db5bc4263b012699b42bdd4f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63010042bc224d51be35b16607c1d59c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da0d8167db324928b946496fbf38ddff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "load_dataset(\"xtreme\", name=\"PAN-X.de\") # Cargar un dataset PAN-X para un idioma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8cc578",
   "metadata": {},
   "source": [
    "Para crear un corpus suizo realista, muestreamos los corpora de alemán (de), francés (fr), italiano (it) y inglés (en) de PAN-X de acuerdo con sus proporciones habladas. Esto creará un desequilibrio lingüístico que es muy común en conjuntos de datos del mundo real, donde adquirir ejemplos etiquetados en una lengua minoritaria puede ser costoso debido a la falta de expertos en la materia que hablen con fluidez ese idioma. Este conjunto de datos desequilibrado simulará una situación común al trabajar en aplicaciones multilingües, y veremos cómo podemos construir un modelo que funcione en todos los idiomas.\n",
    "\n",
    "Para llevar un registro de cada idioma, vamos a crear un defaultdict de Python que almacene el código del idioma como clave y un corpus PAN-X del tipo DatasetDict como valor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3851ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7d4fff3d5bb4966877d1d435c58e72d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PAN-X.fr/train-00000-of-00001.parquet:   0%|          | 0.00/837k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "374b0b2174a649c6a009e9f90737549b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PAN-X.fr/validation-00000-of-00001.parqu(…):   0%|          | 0.00/419k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d352efe6fa45a18031cb48530898a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PAN-X.fr/test-00000-of-00001.parquet:   0%|          | 0.00/423k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5300fe4fe4dd45c3bf6d40b15d6e189b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc8c74bb479434e9c91326bc4ed62ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8e6551e8b9411190d088f8ceedb2e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db167a14c57438db03475144281038b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PAN-X.it/train-00000-of-00001.parquet:   0%|          | 0.00/932k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b25c96dc578e4e2c87d9e88d81f70d8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PAN-X.it/validation-00000-of-00001.parqu(…):   0%|          | 0.00/459k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ce84c95cf3042c4a87005a187852031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PAN-X.it/test-00000-of-00001.parquet:   0%|          | 0.00/464k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d9d5cdff674259a46a07002d6adb53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da46f4f96c3943269369e3146e2aef08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea9fc001b7944d98f2f3fe7072ee64d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad6e7ada869f4021b3c54fab5f14d0db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PAN-X.en/train-00000-of-00001.parquet:   0%|          | 0.00/942k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b1b9a383da4073b3728e8e7525e84e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PAN-X.en/validation-00000-of-00001.parqu(…):   0%|          | 0.00/472k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9049f38f5e69452daaa082d65aa663e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PAN-X.en/test-00000-of-00001.parquet:   0%|          | 0.00/472k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4136e15929fa4cf5856a3a9f16f0ff02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b2cab39567454a9e7a291b76803fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2cedc563644b558beaebeb124ad641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Carga los datasets PAN-X para 4 idiomas (alemán, francés, italiano, inglés) y \n",
    "toma solo una fracción específica de los datos para balancearlos\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from datasets import DatasetDict\n",
    "\n",
    "langs = [\"de\", \"fr\", \"it\", \"en\"]\n",
    "fracs = [0.629, 0.229, 0.084, 0.059]\n",
    "panx_ch = defaultdict(DatasetDict)\n",
    "\n",
    "for lang, frac in zip(langs, fracs):\n",
    "    # Load monolingual corpus\n",
    "    ds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n",
    "    for split in ds:\n",
    "        panx_ch[lang][split] = (\n",
    "        ds[split].shuffle(seed=0).select(range(int(frac * ds[split].num_rows)))\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdae920a",
   "metadata": {},
   "source": [
    "Aquí hemos utilizado el método shuffle() para asegurarnos de que no sesgamos accidentalmente nuestras divisiones de conjuntos de datos, mientras que select() nos permite reducir la muestra de cada corpus de acuerdo con los valores en fracs. Echemos un vistazo a cuántos ejemplos tenemos por idioma en los conjuntos de entrenamiento accediendo al atributo Dataset.num_rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0e37b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>de</th>\n",
       "      <th>fr</th>\n",
       "      <th>it</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Number of training examples</th>\n",
       "      <td>12580</td>\n",
       "      <td>4580</td>\n",
       "      <td>1680</td>\n",
       "      <td>1180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                de    fr    it    en\n",
       "Number of training examples  12580  4580  1680  1180"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame({lang: [panx_ch[lang][\"train\"].num_rows] for lang in langs},index=[\"Number of training examples\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f00aae0",
   "metadata": {},
   "source": [
    "Por diseño, tenemos más ejemplos en alemán que en todos los demás idiomas combinados, por lo que lo usaremos como punto de partida para realizar una transferencia cruzada multilingüe de cero disparos al francés, italiano e inglés. Inspeccionemos uno de los ejemplos en el corpus en alemán:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d17df3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: ['2.000', 'Einwohnern', 'an', 'der', 'Danziger', 'Bucht', 'in', 'der', 'polnischen', 'Woiwodschaft', 'Pommern', '.']\n",
      "ner_tags: [0, 0, 0, 0, 5, 6, 0, 0, 5, 5, 6, 0]\n",
      "langs: ['de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de']\n"
     ]
    }
   ],
   "source": [
    "element = panx_ch[\"de\"][\"train\"][0]\n",
    "for key, value in element.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ecbda9",
   "metadata": {},
   "source": [
    "Al igual que en nuestros encuentros previos con objetos Dataset, las claves de nuestro ejemplo corresponden a los nombres de las columnas de una tabla Arrow, mientras que los valores denotan las entradas en cada columna. En particular, vemos que la columna ner_tags corresponde al mapeo de cada entidad a un ID de clase. Esto es un poco críptico para el ojo humano, así que vamos a crear una nueva columna con las etiquetas familiares LOC, PER y ORG. Para hacer esto, lo primero a notar es que nuestro objeto Dataset tiene un atributo de características que especifica los tipos de datos subyacentes asociados con cada columna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8851f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)\n",
      "ner_tags: Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None), length=-1, id=None)\n",
      "langs: Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)\n"
     ]
    }
   ],
   "source": [
    "for key, value in panx_ch[\"de\"][\"train\"].features.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775a84d1",
   "metadata": {},
   "source": [
    "La clase Sequence especifica que el campo contiene una lista de características, que en el caso de ner_tags corresponde a una lista de características ClassLabel. Seleccionemos esta característica del conjunto de entrenamiento de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94ace59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None)\n"
     ]
    }
   ],
   "source": [
    "tags = panx_ch[\"de\"][\"train\"].features[\"ner_tags\"].feature\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1721937e",
   "metadata": {},
   "source": [
    "Podemos usar el método ClassLabel.int2str() que encontramos en el Capítulo 2 para crear una nueva columna en nuestro conjunto de entrenamiento con los nombres de clase para cada etiqueta. Usaremos el método map() para devolver un diccionario con la clave correspondiente al nuevo nombre de columna y el valor como una lista de nombres de clase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb659978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f651769f7e2466daee92170286400b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12580 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e2050553de14efba14901490d1197b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6290 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df10fab948dc41afb7136cfd8701c956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6290 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_tag_names(batch):\n",
    "    return {\"ner_tags_str\": [tags.int2str(idx) for idx in batch[\"ner_tags\"]]}\n",
    "panx_de = panx_ch[\"de\"].map(create_tag_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc7362e",
   "metadata": {},
   "source": [
    "Ahora que tenemos nuestras etiquetas en un formato legible para humanos, veamos cómo se alinean los tokens y las etiquetas para el primer ejemplo en el conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9099c082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>2.000</td>\n",
       "      <td>Einwohnern</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Danziger</td>\n",
       "      <td>Bucht</td>\n",
       "      <td>in</td>\n",
       "      <td>der</td>\n",
       "      <td>polnischen</td>\n",
       "      <td>Woiwodschaft</td>\n",
       "      <td>Pommern</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0           1   2    3         4      5   6    7           8   \\\n",
       "Tokens  2.000  Einwohnern  an  der  Danziger  Bucht  in  der  polnischen   \n",
       "Tags        O           O   O    O     B-LOC  I-LOC   O    O       B-LOC   \n",
       "\n",
       "                  9        10 11  \n",
       "Tokens  Woiwodschaft  Pommern  .  \n",
       "Tags           B-LOC    I-LOC  O  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_example = panx_de[\"train\"][0]\n",
    "pd.DataFrame([de_example[\"tokens\"], de_example[\"ner_tags_str\"]],\n",
    "['Tokens', 'Tags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addb07bd",
   "metadata": {},
   "source": [
    "La presencia de las etiquetas LOC tiene sentido ya que la frase \"2,000 Einwohnern an der Danziger Bucht in der polnischen Woiwodschaft Pommern\" significa \"2,000 habitantes en la bahía de Gdansk en la voivodía polaca de Pomerania\" en inglés, y la bahía de Gdansk es una bahía en el mar Báltico, mientras que \"voivodía\" corresponde a un estado en Polonia.\n",
    "\n",
    "Como una verificación rápida de que no tenemos ningún desequilibrio inusual en las etiquetas, calculemos las frecuencias de cada entidad en cada división:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fa701a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOC</th>\n",
       "      <th>ORG</th>\n",
       "      <th>PER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>6186</td>\n",
       "      <td>5366</td>\n",
       "      <td>5810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation</th>\n",
       "      <td>3172</td>\n",
       "      <td>2683</td>\n",
       "      <td>2893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>3180</td>\n",
       "      <td>2573</td>\n",
       "      <td>3071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             LOC   ORG   PER\n",
       "train       6186  5366  5810\n",
       "validation  3172  2683  2893\n",
       "test        3180  2573  3071"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Recorre cada partición del dataset (train, test, etc.) y cuenta cuántas veces \n",
    "aparece cada tipo de entidad (como LOC, ORG, PER) al detectar las etiquetas que empiezan por B- (inicio de entidad).\n",
    "\"\"\"\n",
    "from collections import Counter\n",
    "split2freqs = defaultdict(Counter)\n",
    "for split, dataset in panx_de.items():\n",
    "    for row in dataset[\"ner_tags_str\"]:\n",
    "        for tag in row:\n",
    "            if tag.startswith(\"B\"):\n",
    "                tag_type = tag.split(\"-\")[1]\n",
    "                split2freqs[split][tag_type] += 1\n",
    "pd.DataFrame.from_dict(split2freqs, orient=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff93499",
   "metadata": {},
   "source": [
    "Esto se ve bien: las distribuciones de las frecuencias de PER, LOC y ORG son aproximadamente las mismas para cada división, por lo que los conjuntos de validación y prueba deberían proporcionar una buena medida de la capacidad de generalización de nuestro etiquetador NER. A continuación, veamos algunos transformadores multilingües populares y cómo se pueden adaptar para abordar nuestra tarea de NER."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bcaa45",
   "metadata": {},
   "source": [
    "# Multilingual Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d30e2df",
   "metadata": {},
   "source": [
    "Los transformadores multilingües implican arquitecturas y procedimientos de entrenamiento similares a los de sus contrapartes monolingües, excepto que el corpus utilizado para el preentrenamiento consiste en documentos en muchos idiomas. Una característica notable de este enfoque es que, a pesar de no recibir información explícita para diferenciar entre los idiomas, las representaciones lingüísticas resultantes son capaces de generalizar bien entre idiomas para una variedad de tareas posteriores. En algunos casos, esta capacidad de realizar transferencia entre idiomas puede producir resultados que son competitivos con los de los modelos monolingües, lo que elude la necesidad de entrenar un modelo por cada idioma!\n",
    "\n",
    "Para medir el progreso de la transferencia multilingüe para el reconocimiento de entidades nombradas (NER), los conjuntos de datos CoNLL-2002 y CoNLL-2003 se utilizan a menudo como referencia para inglés, neerlandés, español y alemán. Esta referencia consiste en artículos de noticias anotados con las mismas categorías LOC, PER y ORG que PAN-X, pero contiene una etiqueta adicional MISC para entidades diversas que no pertenecen a los tres grupos anteriores. Los modelos de transformadores multilingües suelen ser evaluados de tres maneras diferentes:\n",
    "\n",
    "- `en`: Ajustar finamente los datos de entrenamiento en inglés y luego evaluar en el conjunto de pruebas de cada idioma.\n",
    "- `each`: Ajustar y evaluar en datos de prueba monolingües para medir el rendimiento por idioma.\n",
    "- `all`: Ajustar finamente en todos los datos de entrenamiento para evaluar en cada conjunto de pruebas de cada idioma.\n",
    "\n",
    "Adoptaremos una estrategia de evaluación similar para nuestra tarea de NER, pero primero necesitamos seleccionar un modelo para evaluar. Uno de los primeros transformadores multilingües fue mBERT, que utiliza la misma arquitectura y objetivo de preentrenamiento que BERT, pero añade artículos de Wikipedia de muchos idiomas al corpus de preentrenamiento. Desde entonces, mBERT ha sido superado por XLM-RoBERTa (o XLM-R para abreviar), así que ese es el modelo que consideraremos en este capítulo.\n",
    "\n",
    "Como vimos en el Capítulo 3, XLM-R utiliza solo MLM como un objetivo de preentrenamiento para 100 idiomas, pero se distingue por el enorme tamaño de su corpus de preentrenamiento en comparación con sus precursores: volcado de Wikipedia para cada idioma y 2.5 terabytes de datos de Common Crawl de la web. Este corpus es varios órdenes de magnitud más grande que los utilizados en modelos anteriores y proporciona un impulso significativo en la señal para idiomas de bajos recursos como el birmano y el swahili, donde solo existe un pequeño número de artículos de Wikipedia.\n",
    "\n",
    "La parte del nombre del modelo denominada RoBERTa se refiere al hecho de que el enfoque de preentrenamiento es el mismo que el de los modelos monolingües de RoBERTa. Los desarrolladores de RoBERTa mejoraron varios aspectos de BERT, en particular eliminando por completo la tarea de predicción de la siguiente oración. XLM-R también elimina las incrustaciones de lenguaje usadas en XLM y utiliza SentencePiece para tokenizar los textos sin procesar directamente. Además de su naturaleza multilingüe, una diferencia notable entre XLM-R y RoBERTa es el tamaño de sus respectivos vocabularios: 250,000 tokens frente a 55,000!\n",
    "\n",
    "XLM-R es una excelente opción para tareas de NLU multilingües. En la próxima sección, exploraremos cómo puede tokenizar de manera eficiente en varios idiomas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7aece1",
   "metadata": {},
   "source": [
    "# Una mirada mas cercana a la tokenizacion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e88fc25",
   "metadata": {},
   "source": [
    "En lugar de utilizar un tokenizador WordPiece, XLM-R utiliza un tokenizador llamado SentencePiece que se entrena en el texto en bruto de los cien idiomas. Para entender cómo se compara SentencePiece con WordPiece, carguemos los tokenizadores de BERT y XLM-R de la manera habitual con Transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "621f173b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c45d7e0b9133489b8581195e8c8dfb80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5345c04b9ec4426bef9d008e41f2d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c8819cad9954d84a1e0ee6208fb778a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9211b1fb7b4481d88f5ff46019f4f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3da5645523b46dca6869035a7844f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cb34a3459e244b385d0572dfe3c1f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "924ead3318b846fa828510bf9e9c3e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b7ce19750454d6c8a1d61a42f22804e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "bert_model_name = \"bert-base-cased\"\n",
    "xlmr_model_name = \"xlm-roberta-base\"\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9654287b",
   "metadata": {},
   "source": [
    "Al codificar una pequeña secuencia de texto, también podemos recuperar los tokens especiales que cada modelo utilizó durante el preentrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d99e18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Jack Sparrow loves New York!\"\n",
    "bert_tokens = bert_tokenizer(text).tokens()\n",
    "xlmr_tokens = xlmr_tokenizer(text).tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc5a640",
   "metadata": {},
   "source": [
    "## The Tokenizer Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9841cb82",
   "metadata": {},
   "source": [
    "Hasta ahora, hemos tratado la tokenización como una sola operación que transforma cadenas en enteros que podemos pasar a través del modelo. Esto no es del todo exacto, y si miramos más de cerca, podemos ver que en realidad es un pipeline de procesamiento completo que generalmente consiste en cuatro pasos:\n",
    "1. `Normalization`: Este paso corresponde al conjunto de operaciones que aplicas a una cadena de texto sin procesar para hacerla \"más limpia\". Las operaciones comunes incluyen eliminar espacios en blanco y quitar caracteres acentuados. La normalización de Unicode es otra operación de normalización común aplicada por muchos tokenizadores para abordar el hecho de que a menudo existen varias maneras de escribir el mismo carácter. Esto puede hacer que dos versiones de la \"misma\" cadena (es decir, con la misma secuencia de caracteres abstractos) parezcan diferentes; los esquemas de normalización de Unicode como NFC, NFD, NFKC y NFKD reemplazan las diversas maneras de escribir el mismo carácter con formas estándar. Otro ejemplo de normalización es convertir a minúsculas. Si se espera que el modelo acepte y utilice solo caracteres en minúscula, esta técnica se puede usar para reducir el tamaño del vocabulario que requiere. Después de la normalización, nuestra cadena de ejemplo se vería como \"¡jack sparrow ama nueva york!\".\n",
    "2. `Pretokenization`: Este paso divide un texto en objetos más pequeños que proporcionan un límite superior a lo que serán tus tokens al final del entrenamiento. Una buena manera de pensar en esto es que el pretokenizador dividirá tu texto en \"palabras\", y tus tokens finales serán partes de esas palabras. Para los idiomas que permiten esto (inglés, alemán y muchos idiomas indoeuropeos), las cadenas generalmente pueden dividirse en palabras mediante espacios en blanco y puntuación. Por ejemplo, este paso puede transformar nuestro [\"jack\", \"sparrow\", \"loves\", \"new\", \"york\", \"!\"];.Estas palabras son más simples de dividir en subpalabras con algoritmos de Byte-Pair Encoding (BPE) o Unigram en el siguiente paso de la tubería. Sin embargo, dividir en \"palabras\" no siempre es una operación trivial y determinista, o incluso una operación que tenga sentido. Por ejemplo, en idiomas como el chino, japonés o coreano, agrupar símbolos en unidades semánticas como las palabras indoeuropeas puede ser una operación no determinista con varios grupos igualmente válidos. En este caso, podría ser mejor no pretokenizar el texto y, en su lugar, utilizar una biblioteca específica para el idioma para la pretokenización.\n",
    "3. `Tokenizer model`: Una vez que los textos de entrada están normalizados y pretokens, el tokenizador aplica un modelo de división de subpalabras sobre las palabras. Esta es la parte del proceso que necesita ser entrenada en su corpus (o que ha sido entrenada si está utilizando un tokenizador preentrenado). El papel del modelo es dividir las palabras en subpalabras para reducir el tamaño del vocabulario y tratar de disminuir el número de tokens fuera de vocabulario. Existen varios algoritmos de tokenización de subpalabras, incluyendo BPE, Unigram y WordPiece. Por ejemplo, nuestro ejemplo en funcionamiento podría verse como [jack, spa, rrow, loves, new, york, !] después de aplicar el modelo tokenizador. Tenga en cuenta que en este punto ya no tenemos una lista de cadenas sino una lista de enteros (ID de entrada); para mantener el ejemplo ilustrativo, hemos conservado las palabras pero hemos quitado las comillas para indicar la transformación.\n",
    "4. `Postprocessing`:Este es el último paso del proceso de tokenización, en el cual se pueden aplicar algunas transformaciones adicionales a la lista de tokens; por ejemplo, agregar tokens especiales al principio o al final de la secuencia de entrada de índices de tokens. Por ejemplo, un tokenizador estilo BERT agregaría tokens de clasificación y separación: [CLS, jack, spa, rrow, ama, nueva, york, !, SEP]. Esta secuencia (recuerda que será una secuencia de números enteros, no los tokens que ves aquí) puede ser alimentada al modelo.\n",
    "\n",
    "Volviendo a nuestra comparación de XLM-R y BERT, ahora entendemos que SentencePiece agrega \"s y \\s\" en lugar de [CLS] y [SEP] en el paso de posprocesamiento (como convención, continuaremos usando [CLS] y [SEP] en las ilustraciones gráficas). Volvamos al tokenizador SentencePiece para ver qué lo hace especial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7d790d",
   "metadata": {},
   "source": [
    "## El tokenizador SentencePiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d96a27",
   "metadata": {},
   "source": [
    "El tokenizador SentencePiece se basa en un tipo de segmentación de subpalabras llamada Unigram y codifica cada texto de entrada como una secuencia de caracteres Unicode. Esta última característica es especialmente útil para corpora multilingües, ya que permite que SentencePiece sea agnóstico sobre acentos, puntuación y el hecho de que muchos idiomas, como el japonés, no tienen caracteres de espacio en blanco. Otra característica especial de SentencePiece es que el espacio en blanco se asigna al símbolo Unicode U+2581, o al carácter , también llamado el carácter de bloque inferior.\n",
    "\n",
    "Esto permite que SentencePiece des-tokenice una secuencia sin ambigüedades y sin depender de pre-tokenizadores específicos del idioma. En nuestro ejemplo de la sección anterior, por ejemplo, podemos ver que WordPiece ha perdido la información de que no hay espacio en blanco entre \"York\" y \"!\". Por el contrario, SentencePiece preserva el espacio en blanco en el texto tokenizado, por lo que podemos convertir de nuevo al texto sin procesar sin ambigüedad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fe60593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Jack Sparrow loves New York!</s>'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(xlmr_tokens).replace(u\"\\u2581\", \" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
