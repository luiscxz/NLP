{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aec0e21",
   "metadata": {},
   "source": [
    "**Tabla de contenido**\n",
    "\n",
    "- [Text Classification](#Text-Classification)\n",
    "    - [The Dataset](#The-Dataset)\n",
    "        - [A First Look at Hugging Face Datasets](#A-First-Look-at-Hugging-Face-Datasets)\n",
    "        - [From Datasets to DataFrames](#From-Datasets-to-DataFrames)\n",
    "        - [Looking at the Class Distribution](#Looking-at-the-Class-Distribution)\n",
    "        - [How Long Are Our Tweets?](#How-Long-Are-Our-Tweets?)\n",
    "    - [From Text to Tokens](#From-Text-to-Tokens)\n",
    "        - [Character Tokenization](#Character-Tokenization)\n",
    "        - [Word Tokenization](#Word-Tokenization)\n",
    "        - [Subword Tokenization](#Subword-Tokenization)\n",
    "        - [Tokenizing the Whole Dataset](#Tokenizing-the-Whole-Dataset)\n",
    "    - [Training a Text Classifier](#Training-a-Text-Classifier)\n",
    "        - [Transformers as Feature Extractors](#Transformers-as-Feature-Extractors)\n",
    "            - [Using pretrained models](#Using-pretrained-models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfac6de1",
   "metadata": {},
   "source": [
    "# Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3b6815",
   "metadata": {},
   "source": [
    "`La clasificación de texto es una de las tareas más comunes en el procesamiento del lenguaje natural (NLP)`; se puede utilizar para una amplia gama de aplicaciones, como `clasificar comentarios` de clientes en categorías o dirigir tickets de soporte según su idioma. ¡Es probable que el filtro de spam de tu programa de correo electrónico esté utilizando clasificación de texto para proteger tu bandeja de entrada de una avalancha de correo no deseado!\n",
    "\n",
    "Otro tipo común de `clasificación de textos es el análisis de sentimientos`, que (como vimos en el Capítulo 1) tiene como objetivo identificar la polaridad de un texto dado.\n",
    "\n",
    "Ahora imagina que eres un científico de datos que necesita construir un sistema que pueda identificar automáticamente estados emocionales como \"ira\" o \"alegría\" que las personas expresan sobre el producto de tu empresa en Twitter. En este capítulo, abordaremos esta tarea utilizando una variante de BERT llamada `DistilBERT`.\n",
    "\n",
    "La principal ventaja de este modelo es que `logra un rendimiento comparable al de BERT`, a la vez que es significativamente más pequeño y eficiente. Esto nos permite entrenar un clasificador en unos pocos minutos, y si deseas entrenar un modelo BERT más grande, simplemente puedes cambiar el `checkpoint` del modelo preentrenado. Un `checkpoint` corresponde al `conjunto de pesos que se cargan en una arquitectura de transformador dada`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9af508",
   "metadata": {},
   "source": [
    "## The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8fab9a",
   "metadata": {},
   "source": [
    "Para construir nuestro detector de emociones utilizaremos un gran conjunto de datos de un artículo que exploró cómo se representan las emociones en los mensajes de Twitter en inglés. A diferencia de la mayoría de los conjuntos de datos de análisis de sentimientos que solo involucran polaridades “positivas” y “negativas”, `este conjunto de datos contiene seis emociones básicas: ira, asco, miedo, alegría, tristeza y sorpresa`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad7fb82",
   "metadata": {},
   "source": [
    "### A First Look at Hugging Face Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496abba5",
   "metadata": {},
   "source": [
    "Usaremos Datasets para descargar los datos del Hugging Face Hub. Podemos usar la función list_datasets() para ver qué datasets están disponibles en el Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a97976d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10 datasets currently available on the Hub\n",
      "The first 10 are: ['institutional/institutional-books-1.0', 'nvidia/Nemotron-Personas', 'fka/awesome-chatgpt-prompts', 'open-thoughts/OpenThoughts3-1.2M', 'openbmb/Ultra-FineWeb']\n"
     ]
    }
   ],
   "source": [
    "#pip install huggingface-hub\n",
    "from huggingface_hub import list_datasets\n",
    "\n",
    "try:\n",
    "    # Limitar a los primeros 100 datasets \n",
    "    datasets = list(list_datasets(limit=10))\n",
    "    \n",
    "    total = len(datasets)\n",
    "    first_5 = [ds.id for ds in datasets[:5]]\n",
    "    \n",
    "    print(f\"There are {total} datasets currently available on the Hub\")\n",
    "    print(f\"The first 10 are: {first_5}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ddc1c8",
   "metadata": {},
   "source": [
    "Vemos que a `cada conjunto de datos se le da un nombre`, así que carguemos el conjunto de datos de `emociones` con la función load_dataset():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60c2e91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "emotions = load_dataset(\"emotion\")\n",
    "emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3ca4bf",
   "metadata": {},
   "source": [
    "vemos que `es similar a un diccionario de Python, donde cada clave corresponde a una división diferente`. Y podemos usar la sintaxis habitual de diccionario para acceder a una división individual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab89ff81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'label']\n",
      "{'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'], id=None)}\n",
      "{'text': ['i didnt feel humiliated', 'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake', 'im grabbing a minute to post i feel greedy wrong', 'i am ever feeling nostalgic about the fireplace i will know that it is still on the property', 'i am feeling grouchy'], 'label': [0, 0, 3, 2, 3]}\n"
     ]
    }
   ],
   "source": [
    "train_ds = emotions[\"train\"]\n",
    "print(train_ds.column_names) # nombre de las columnas\n",
    "print(train_ds.features) # imprime las clases que hay \n",
    "print(train_ds[:5]) # imprime los 5 primeros registros, con sus respectivas clases (label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e434ab7",
   "metadata": {},
   "source": [
    "Tenga en cuenta que en este caso, `los valores del diccionario son ahora listas en lugar de elementos individuales`. También podemos obtener la columna completa por nombre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9db3faa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i didnt feel humiliated', 'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake', 'im grabbing a minute to post i feel greedy wrong', 'i am ever feeling nostalgic about the fireplace i will know that it is still on the property', 'i am feeling grouchy']\n"
     ]
    }
   ],
   "source": [
    "print(train_ds[\"text\"][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225e0345",
   "metadata": {},
   "source": [
    "### From Datasets to DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fdcef2",
   "metadata": {},
   "source": [
    "Aunque Datasets proporciona mucha funcionalidad de bajo nivel para segmentar y manipular nuestros datos, a menudo es conveniente convertir un objeto Dataset en un DataFrame de Pandas para que podamos acceder a APIs de alto nivel para la visualización de datos. Para habilitar la conversión, sets proporciona un método set_format() que nos permite cambiar el formato de salida del Dataset. Tenga en cuenta que esto no cambia el formato de datos subyacente (que es una tabla Arrow), y puede cambiar a otro formato más adelante si es necesario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "850d7889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                            i didnt feel humiliated      0\n",
       "1  i can go from feeling so hopeless to so damned...      0\n",
       "2   im grabbing a minute to post i feel greedy wrong      3\n",
       "3  i am ever feeling nostalgic about the fireplac...      2\n",
       "4                               i am feeling grouchy      3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "emotions.set_format(type=\"pandas\")\n",
    "df = emotions[\"train\"][:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d701feb5",
   "metadata": {},
   "source": [
    "Como puedes ver, `los encabezados de las columnas se han conservado` y las primeras filas coinciden con nuestras vistas anteriores de los datos. `Sin embargo, las etiquetas se representan como enteros`, así que usemos el `método int2str()` de la característica de etiqueta `para crear una nueva columna en nuestro DataFrame con los nombres de etiqueta correspondientes`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69b76aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label label_name\n",
       "0                            i didnt feel humiliated      0    sadness\n",
       "1  i can go from feeling so hopeless to so damned...      0    sadness\n",
       "2   im grabbing a minute to post i feel greedy wrong      3      anger\n",
       "3  i am ever feeling nostalgic about the fireplac...      2       love\n",
       "4                               i am feeling grouchy      3      anger"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def label_int2str(row):\n",
    "    return emotions[\"train\"].features[\"label\"].int2str(row)\n",
    "df[\"label_name\"] = df[\"label\"].apply(label_int2str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaf809f",
   "metadata": {},
   "source": [
    "Antes de sumergirnos en la construcción de un clasificador, echemos un vistazo más de cerca al conjunto de datos. `Como señala Andrej Karpathy` en su famoso blog `“Una receta para entrenar redes neuronales”`, convertirse en `“uno con los datos”` es un paso esencial para entrenar grandes modelos!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1484e869",
   "metadata": {},
   "source": [
    "### Looking at the Class Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557b021a",
   "metadata": {},
   "source": [
    "`Siempre que estés trabajando en problemas de clasificación de texto`, es una buena idea `examinar la distribución de ejemplos en las clases`. Un conjunto de datos con una distribución de clases sesgada podría requerir un tratamiento diferente en términos de pérdida de entrenamiento y métricas de evaluación que uno balanceado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "476c7f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df[\"label_name\"].value_counts(ascending=True).plot.barh()\n",
    "plt.title(\"Frequency of Classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d1fd3c",
   "metadata": {},
   "source": [
    "En este caso, podemos ver que `el conjunto de datos está muy desbalanceado`; las clases de alegría y tristeza aparecen con frecuencia, mientras que amor y sorpresa son aproximadamente 5 a 10 veces más raras. Existen varias maneras de lidiar con datos desbalanceados, incluyendo:\n",
    "\n",
    "- Sobremuestrear aleatoriamente la clase minoritaria.\n",
    "- Submuestrear aleatoriamente la clase mayoritaria.\n",
    "- Reunir más datos etiquetados de las clases subrepresentadas.\n",
    "\n",
    "Si deseas aprender más sobre estas técnicas de muestreo, te recomendamos consultar la biblioteca `Imbalanced-learn`. `¡Solo asegúrate de no aplicar métodos de muestreo antes de crear tus divisiones de entrenamiento/prueba`, o tendrás mucha fuga entre ellas!\n",
    "\n",
    "Ahora que hemos visto las clases, echemos un vistazo a los tweets en sí."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cb90e8",
   "metadata": {},
   "source": [
    "### How Long Are Our Tweets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9cbe42",
   "metadata": {},
   "source": [
    "`Los modelos de transformadores tienen una longitud máxima de secuencia de entrada` que se conoce como el tamaño máximo del contexto `(maximum context size)`. Para las aplicaciones que utilizan DistilBERT, el tamaño máximo del contexto es de 512 tokens, lo que equivale a unos pocos párrafos de texto. Como veremos en la siguiente sección, un token es una pieza atómica de texto; por ahora, trataremos un token como una sola palabra. `Podemos obtener una estimación aproximada de las longitudes de los tuits por emoción observando la distribución de palabras por tuit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4218fe2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"Words Per Tweet\"] = df[\"text\"].str.split().apply(len)\n",
    "df.boxplot(\"Words Per Tweet\", by=\"label_name\", grid=False,\n",
    "showfliers=False, color=\"black\")\n",
    "plt.suptitle(\"\")\n",
    "plt.xlabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d29eef",
   "metadata": {},
   "source": [
    "A partir del gráfico, vemos que `para cada emoción, la mayoría de los tweets tienen alrededor de 15 palabras de longitud` y `los tweets más largos están muy por debajo del tamaño máximo de contexto de DistilBERT`. Los textos que son más largos que el tamaño de contexto de un modelo necesitan ser truncados, lo que puede llevar a una pérdida de rendimiento si el texto truncado contiene información crucial; en este caso, parece que eso no será un problema.\n",
    "\n",
    "`¡Ahora descubramos cómo podemos convertir estos textos en bruto en un formato adecuado para Transformers!` Mientras estamos en eso, `también reiniciemos el formato de salida de nuestro conjunto de datos ya que no necesitamos más el formato de DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "664a9456",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520ce2fc",
   "metadata": {},
   "source": [
    "## From Text to Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af8a4ed",
   "metadata": {},
   "source": [
    "`Los modelos de transformadores como DistilBERT no pueden recibir cadenas de texto sin procesar como entrada; en su lugar, asumen que el texto ha sido tokenizado y codificado como vectores numéricos`. `La tokenización es el paso de descomponer una cadena en las unidades  utilizadas en el modelo`. Hay varias estrategias de tokenización que se pueden adoptar, y la división óptima de las palabras en subunidades generalmente se aprende del corpus. Antes de mirar el tokenizador utilizado para DistilBERT, consideremos dos casos extremos: `la tokenización a nivel de caracteres` y `la tokenización a nivel de palabras`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f046fd3",
   "metadata": {},
   "source": [
    "### Character Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5113b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'o', 'k', 'e', 'n', 'i', 'z', 'i', 'n', 'g', ' ', 't', 'e', 'x', 't', ' ', 'i', 's', ' ', 'a', ' ', 'c', 'o', 'r', 'e', ' ', 't', 'a', 's', 'k', ' ', 'o', 'f', ' ', 'N', 'L', 'P', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Tokenizing text is a core task of NLP.\"\n",
    "tokenized_text = list(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338fed62",
   "metadata": {},
   "source": [
    "Este es un buen comienzo, pero aún no hemos terminado. `Nuestro modelo espera que cada carácter se convierta en un entero`, un proceso que a veces se llama numerización. Una forma sencilla de hacer esto es codificando cada token único (que son caracteres en este caso) con un entero único:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "690dc23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, '.': 1, 'L': 2, 'N': 3, 'P': 4, 'T': 5, 'a': 6, 'c': 7, 'e': 8, 'f': 9, 'g': 10, 'i': 11, 'k': 12, 'n': 13, 'o': 14, 'r': 15, 's': 16, 't': 17, 'x': 18, 'z': 19}\n"
     ]
    }
   ],
   "source": [
    "token2idx = {ch: idx for idx, ch in enumerate(sorted(set(tokenized_text)))}\n",
    "print(token2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ed61a4",
   "metadata": {},
   "source": [
    "Esto nos da un mapeo de cada carácter en nuestro vocabulario a un entero único. Ahora podemos usar token2idx para transformar el texto tokenizado en una lista de enteros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "124ed27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 14, 12, 8, 13, 11, 19, 11, 13, 10, 0, 17, 8, 18, 17, 0, 11, 16, 0, 6, 0, 7, 14, 15, 8, 0, 17, 6, 16, 12, 0, 14, 9, 0, 3, 2, 4, 1]\n"
     ]
    }
   ],
   "source": [
    "input_ids = [token2idx[token] for token in tokenized_text]\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800034f8",
   "metadata": {},
   "source": [
    "Cada token ahora se ha mapeado a un identificador numérico único (de ahí el nombre input_ids). El último paso es convertir input_ids en un tensor 2D de vectores one-hot. Los vectores one-hot se utilizan con frecuencia en el aprendizaje automático para codificar datos categóricos, que pueden ser ordinales o nominales. Por ejemplo, supongamos que queremos codificar los nombres de los personajes de la serie de televisión Transformers. Una forma de hacer esto sería mapear cada nombre a una ID única, de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7f7d044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Label ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bumblebee</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Optimus Prime</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Megatron</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Name  Label ID\n",
       "0      Bumblebee         0\n",
       "1  Optimus Prime         1\n",
       "2       Megatron         2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_df = pd.DataFrame(\n",
    "{\"Name\": [\"Bumblebee\", \"Optimus Prime\", \"Megatron\"], \"Label ID\": [0,1,2]})\n",
    "categorical_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041a2e2f",
   "metadata": {},
   "source": [
    "El problema con este enfoque es que crea un orden ficticio entre los nombres, y las redes neuronales son muy buenas para aprender este tipo de relaciones. Así que, en lugar de eso, podemos crear una nueva columna para cada categoría y asignar un 1 donde la categoría sea verdadera y un 0 en caso contrario. En Pandas, esto se puede implementar con la función get_dummies() de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7f913a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bumblebee</th>\n",
       "      <th>Megatron</th>\n",
       "      <th>Optimus Prime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Bumblebee  Megatron  Optimus Prime\n",
       "0          1         0              0\n",
       "1          0         0              1\n",
       "2          0         1              0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(categorical_df[\"Name\"],dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc69a8b7",
   "metadata": {},
   "source": [
    "Podemos crear las codificaciones one-hot en PyTorch convirtiendo `input_ids` en un tensor y aplicando la función one_hot() de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a21ed37a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([38, 20])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "input_ids = torch.tensor(input_ids)\n",
    "one_hot_encodings = F.one_hot(input_ids, num_classes=len(token2idx))\n",
    "one_hot_encodings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae0c59f",
   "metadata": {},
   "source": [
    "`Para cada uno de los 38 tokens de entrada, ahora tenemos un vector one-hot con 20 dimensiones, ya que nuestro vocabulario consiste en 20 caracteres únicos.`\n",
    "\n",
    "`Nota`: `Es importante siempre establecer num_classes en la función one_hot()` porque de lo contrario los vectores one-hot pueden terminar siendo más cortos que la longitud del vocabulario (y necesitarán ser rellenados con ceros manualmente). En TensorFlow, la función equivalente es tf.one_hot(), donde el argumento depth cumple el papel de num_classes.\n",
    "\n",
    "Al examinar el primer vector, podemos verificar que un 1 aparece en la ubicación indicada por input_ids[0]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f33ffd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: T\n",
      "Tensor index: 5\n",
      "One-hot: tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Token: {tokenized_text[0]}\")\n",
    "print(f\"Tensor index: {input_ids[0]}\")\n",
    "print(f\"One-hot: {one_hot_encodings[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5e2460",
   "metadata": {},
   "source": [
    "A partir de nuestro simple ejemplo, podemos ver que la tokenización a nivel de caracteres ignora cualquier estructura en el texto y trata toda la cadena como un flujo de caracteres. `Aunque esto ayuda a lidiar con errores ortográficos y palabras raras, la principal desventaja es que las estructuras lingüísticas, como las palabras, necesitan ser aprendidas a partir de los datos. Esto requiere un uso significativo de cómputo, memoria y datos`. Por esta razón, `la tokenización de caracteres rara vez se utiliza en la práctica`. En cambio, se preserva alguna estructura del texto durante el proceso de tokenización. La tokenización de palabras es un enfoque simple para lograr esto, así que echemos un vistazo a cómo funciona."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489a29d4",
   "metadata": {},
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027d8dbd",
   "metadata": {},
   "source": [
    "`En lugar de dividir el texto en caracteres, podemos dividirlo en palabras y mapear cada palabra a un número entero`. `Utilizar palabras desde el principio permite al modelo omitir el paso de aprender palabras a partir de caracteres`, y así reduce la complejidad del proceso de entrenamiento.\n",
    "\n",
    "`Una clase simple de tokenizadores de palabras utiliza el espacio en blanco para tokenizar el texto`. Podemos hacer esto aplicando la función split() de Python directamente en el texto bruto (tal como lo hicimos para medir las longitudes de los tweets):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a59ebba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenizing', 'text', 'is', 'a', 'core', 'task', 'of', 'NLP.']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = text.split()\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1d1f4c",
   "metadata": {},
   "source": [
    "Desde aquí podemos seguir los mismos pasos que tomamos para el tokenizador de caracteres para mapear cada palabra a un ID. Sin embargo, ya `podemos ver un posible problema con este esquema de tokenización`: `la puntuación no se tiene en cuenta`, por lo que NLP. se trata como un único token. `Dado que las palabras pueden incluir declinaciones, conjugaciones o errores ortográficos`, `¡el tamaño del vocabulario puede crecer fácilmente hasta millones!`\n",
    "\n",
    "`Nota`: `Algunos tokenizadores de palabras tienen reglas adicionales para la puntuación`. También se puede aplicar el `stemming` o la `lemmatization`, que normaliza las palabras a su raíz (por ejemplo, \"grande\", \"mayor\" y \"el más grande\" se convierten en \"grande\"), a costa de perder algo de información en el texto.\n",
    "\n",
    "Tener un vocabulario grande es un problema porque requiere que las redes neuronales tengan un número enorme de parámetros. Naturalmente, queremos evitar ser tan derrochadores con los parámetros de nuestro modelo, ya que los modelos son costosos de entrenar y los modelos más grandes son más difíciles de mantener. \n",
    "\n",
    "`Un enfoque común es limitar el vocabulario y descartar palabras raras considerando, por ejemplo, las 100,000 palabras más comunes en el corpus`. Las palabras que no forman parte del vocabulario se clasifican como \"desconocidas\" y se asignan a un token compartido UNK. Esto significa que perdemos información potencialmente importante en el proceso de tokenización de palabras, ya que el modelo no tiene información sobre las palabras asociadas con UNK.\n",
    "\n",
    "¿No sería genial si hubiera un compromiso entre la tokenización de caracteres y la tokenización de palabras que preservara toda la información de entrada y parte de la estructura de entrada? Sí lo hay: `la tokenización de subpalabras.`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b421220b",
   "metadata": {},
   "source": [
    "### Subword Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464efcee",
   "metadata": {},
   "source": [
    "`La idea básica detrás de la tokenización de subpalabras es combinar los mejores aspectos de la tokenización por caracteres y palabras.` Por un lado, queremos `dividir palabras raras en unidades más pequeñas` para permitir que `el modelo lidie con palabras complejas y errores ortográficos`. Por otro lado, queremos mantener palabras frecuentes como entidades únicas para que podamos mantener la longitud de nuestras entradas a un tamaño manejable. `La principal característica distintiva de la tokenización de subpalabras` (así como de la tokenización por palabras) `es que se aprende del corpus de preentrenamiento utilizando una mezcla de reglas y algoritmos estadísticos`.\n",
    "\n",
    "`Hay varios algoritmos de tokenización de subpalabras que se utilizan comúnmente en NLP`, pero comencemos con `WordPiece`, que es utilizado por los tokenizadores de BERT y DistilBERT. La forma más fácil de entender cómo funciona WordPiece es verlo en acción. Transformers proporciona una conveniente clase AutoTokenizer que te permite cargar rápidamente el tokenizador asociado con un modelo preentrenado: solo llamamos a su método from_pretrained(), proporcionando la ID de un modelo en el Hub o una ruta de archivo local. Comencemos cargando el tokenizador para DistilBERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea9d1d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbce9ff95dea487187eeda8ece4753bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12df34b49694f70b4ffc081dbcf9454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2578d1b9ec034135ba19c3e2567d7b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1577fd607272456ca22ab56a1195fd42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69870a0b",
   "metadata": {},
   "source": [
    "La clase `AutoTokenizer` pertenece a un conjunto más amplio de clases \"auto\", `cuyo trabajo es recuperar automáticamente la configuración del modelo`, `los pesos preentrenados o el vocabulario a partir del nombre del punto de control`. Esto te permite cambiar rápidamente entre modelos, pero si deseas cargar la clase específica manualmente, también puedes hacerlo. Por ejemplo, podríamos haber cargado el tokenizador DistilBERT de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83aee38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c6e661",
   "metadata": {},
   "source": [
    "Examinemos cómo funciona este tokenizador alimentándolo con nuestro simple ejemplo de “Tokenizing text is a core task of NLP.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c5a93aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 19204, 6026, 3793, 2003, 1037, 4563, 4708, 1997, 17953, 2361, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded_text = tokenizer(text)\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533f1598",
   "metadata": {},
   "source": [
    "Al igual que con la tokenización de caracteres, podemos ver que las palabras han sido mapeadas a enteros únicos en el campo input_ids. Discutiremos el papel del campo attention_mask en la siguiente sección. `Ahora que tenemos los input_ids, podemos convertirlos de nuevo en tokens usando el método convert_ids_to_tokens() del tokenizador:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f30244e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'token', '##izing', 'text', 'is', 'a', 'core', 'task', 'of', 'nl', '##p', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(encoded_text.input_ids)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d4c0e2",
   "metadata": {},
   "source": [
    "`Podemos observar tres cosas aquí`: \n",
    "- Primero, se han agregado algunos `tokens especiales` `[CLS]` y `[SEP]` al `inicio` y al `final` de la secuencia. Estos tokens difieren de un modelo a otro, pero `su función principal es indicar el inicio y el final de una secuencia`.\n",
    "\n",
    "- En segundo lugar, `los tokens han sido convertidos a minúsculas`, lo cual es una característica de este punto de control en particular. Finalmente, podemos ver que `\"tokenizing\"` y `\"NLP\"` se han dividido en dos tokens, lo que tiene sentido dado que no son palabras comunes.\n",
    "\n",
    "`El prefijo ##` en ##izando y ##p `significa que la cadena anterior no es un espacio en blanco`; cualquier token con este prefijo debe fusionarse con el token anterior cuando conviertas los tokens de nuevo a una cadena. La clase AutoTokenizer tiene un método convert_tokens_to_string() para hacer exactamente eso, así que apliquémoslo a nuestros tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66b82de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] tokenizing text is a core task of nlp. [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_tokens_to_string(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14263f1",
   "metadata": {},
   "source": [
    "La clase AutoTokenizer también tiene varios atributos que proporcionan información sobre el tokenizador. Por ejemplo, podemos inspeccionar el tamaño del vocabulario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1611277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac353a3",
   "metadata": {},
   "source": [
    "y el tamaño máximo de contexto del modelo correspondiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06373f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5e2f13",
   "metadata": {},
   "source": [
    "Otro atributo interesante que hay que conocer son los nombres de los campos que el modelo espera en su paso hacia adelante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "900450f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'attention_mask']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_input_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72733e1",
   "metadata": {},
   "source": [
    "Ahora que tenemos una comprensión básica del proceso de tokenización para una sola cadena, `¡veamos cómo podemos tokenizar todo el conjunto de datos!`\n",
    "\n",
    "`Nota`: Al usar modelos preentrenados, es muy importante asegurarse de usar el mismo tokenizador con el que se entrenó el modelo. Desde la perspectiva del modelo, cambiar el tokenizador es como barajar el vocabulario. Si todos a tu alrededor comenzaran a intercambiar palabras aleatorias como “casa” por “gato,” también tendrías dificultades para entender lo que está sucediendo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2490a844",
   "metadata": {},
   "source": [
    "### Tokenizing the Whole Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771324d3",
   "metadata": {},
   "source": [
    "`Para tokenizar todo el corpus, usaremos el método map()` de nuestro objeto DatasetDict. Encontraremos este método muchas veces a lo largo de este libro, ya que proporciona una forma conveniente de aplicar una función de procesamiento a cada elemento en un conjunto de datos. Como pronto veremos, `el método map() también se puede usar para crear nuevas filas y columnas`.\n",
    "\n",
    "Para empezar, lo primero que necesitamos es una función de procesamiento para tokenizar nuestros ejemplos.\n",
    "\n",
    "Esta función aplica el tokenizador a un lote de ejemplos; `padding=True` rellenará los ejemplos con ceros hasta el tamaño más largo en un lote, y `truncation=True` truncará los ejemplos hasta el tamaño máximo de contexto del modelo. Para ver tokenize() en acción, pasemos un lote de dos ejemplos del conjunto de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e6fe857f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1045, 2134, 2102, 2514, 26608, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1045, 2064, 2175, 2013, 3110, 2061, 20625, 2000, 2061, 9636, 17772, 2074, 2013, 2108, 2105, 2619, 2040, 14977, 1998, 2003, 8300, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "print(tokenize(emotions[\"train\"][:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a278d9a1",
   "metadata": {},
   "source": [
    "Aquí podemos ver el resultado del padding: el primer elemento de input_ids es más corto que el segundo, por lo que se han añadido ceros a ese elemento para que tengan la misma longitud. Estos ceros tienen un token [PAD] correspondiente en el vocabulario, y el conjunto de tokens especiales también incluye los tokens [CLS] y [SEP] que encontramos anteriormente.\n",
    "\n",
    "`También ten en cuenta que, además de devolver los tweets codificados como input_ids`, `el tokenizador devuelve una lista de arreglos de attention_mask`. Esto se debe a que no queremos que el modelo se confunda por los tokens de relleno adicionales: la máscara de atención permite al modelo ignorar las partes rellenadas de la entrada. La siguiente figura proporciona una explicación visual de cómo se rellenan los input IDs y las máscaras de atención.\n",
    "![Texto alternativo](images/token.png)\n",
    "\n",
    "*Para cada lote, las secuencias de entrada se rellenan hasta la longitud máxima de secuencia en el lote; la máscara de atención se utiliza en el modelo para ignorar las áreas rellenadas de los tensores de entrada.*\n",
    "\n",
    "Una vez que hayamos definido una función de procesamiento, podemos aplicarla a todas las divisiones del corpus en una sola línea de código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aafb225d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b514e02a6c6043218742de8cad191ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052db9e152f44086b0e143b68a9c0070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "522c77c95de243ad88685a354b5c8e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45bc175",
   "metadata": {},
   "source": [
    "Por defecto, el `método map()` opera individualmente en cada ejemplo del corpus, por lo que establecer `batched=True` `codificará los tweets en lotes`. Dado que hemos establecido `batch_size=None`, `nuestra función tokenize() se aplicará a todo el conjunto de datos como un solo lote`. `Esto asegura que los tensores de entrada y las máscaras de atención tengan la misma forma a nivel global`, y podemos ver que esta operación ha añadido nuevas columnas input_ids y attention_mask al conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4df85576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'label', 'input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "print(emotions_encoded[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df136df2",
   "metadata": {},
   "source": [
    "## Training a Text Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd141bd3",
   "metadata": {},
   "source": [
    "Como se discutió en el Capítulo 1, `modelos como DistilBERT están preentrenados para predecir palabras enmascaradas en una secuencia de texto`. `Sin embargo, no podemos usar estos modelos de lenguaje directamente para la clasificación de texto`; `necesitamos modificarlos ligeramente`. Para comprender qué modificaciones son necesarias, echemos un vistazo a la arquitectura de un modelo basado en codificadores como DistilBERT, que se representa en la siguiente figura.\n",
    "![aquitectura modelo basado en codificadores](images/arquitecturamodelo.png)\n",
    "*La arquitectura utilizada para la clasificación de secuencias con un transformador basado en un codificador; consiste en el cuerpo preentrenado del modelo combinado con una cabeza de clasificación personalizada.*\n",
    "\n",
    "Primero, el texto se tokeniza y se representa como vectores one-hot llamados codificaciones de tokens. El tamaño del vocabulario del tokenizador determina la dimensión de las codificaciones de tokens, y generalmente consiste en 20k a 200k tokens únicos. A continuación, estas codificaciones de tokens se convierten en token embeddings, que son vectores que viven en un espacio de menor dimensión. Luego, los token embeddings se pasan a través de las capas del bloque del codificador para generar un `hidden state` para cada token de entrada. `Para el objetivo de preentrenamiento de modelado de lenguaje`, cada `hidden state` se alimenta a una capa que predicew los `masked input tokens`. Para la tarea de clasificación, reemplazamos la capa de modelado de lenguaje por una capa de clasificación.\n",
    "\n",
    "`Nota`: En la práctica, PyTorch omite el paso de crear vectores one-hot para las codificaciones de tokens, porque multiplicar una matriz con un vector one-hot es lo mismo que seleccionar una columna de la matriz. Esto se puede hacer directamente obteniendo la columna con el ID del token de la matriz. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4486cbb",
   "metadata": {},
   "source": [
    "`Tenemos dos opciones para entrenar un modelo así en nuestro conjunto de datos de Twitter:`\n",
    "- **Feature extraction**: Utilizamos los estados ocultos (hidden states) como características y simplemente entrenamos un clasificador sobre ellos, sin modificar el modelo preentrenado.\n",
    "- **Fine-tuning**: Entrenamos todo el modelo de extremo a extremo, lo que también actualiza los parámetros del modelo preentrenado.\n",
    "\n",
    "`En las siguientes secciones exploramos ambas opciones para DistilBERT y examinamos sus compensaciones.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485094d9",
   "metadata": {},
   "source": [
    "### Transformers as Feature Extractors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a44822",
   "metadata": {},
   "source": [
    "`Usar un transformador como extractor de características es bastante simple`. Como se muestra en la siguiente Figura, congelamos los pesos del cuerpo durante el entrenamiento y usamos los estados ocultos como características para el clasificador. `La ventaja de este enfoque es que podemos entrenar rápidamente un modelo pequeño o superficial`. Tal modelo podría ser una capa de clasificación neuronal o un método que no dependa de gradientes, como un bosque aleatorio. `Este método es especialmente conveniente si las GPU no están disponibles,` ya que los estados ocultos solo necesitan ser precomputados una vez.\n",
    "\n",
    "![features stractor](images/featurestractor.png)\n",
    "*En el enfoque basado en características, el modelo DistilBERT está congelado y solo proporciona características para un clasificador.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b947ab",
   "metadata": {},
   "source": [
    "#### Using pretrained models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
