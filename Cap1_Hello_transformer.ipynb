{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a09a1f06",
   "metadata": {},
   "source": [
    "**Tabla de contenido Capitulo 1**\n",
    "\n",
    "- [Hello Transformers](#Hello-Transformers)\n",
    "    - [A Tour of Transformer Applications](#A-Tour-of-Transformer-Applications)\n",
    "    - [Text Classification](#Text-Classification)\n",
    "    - [Named Entity Recognition](#Named-Entity-Recognition)\n",
    "    - [Question Answering](#Question-Answering)\n",
    "    - [Summarization](#Summarization)\n",
    "    - [Translation](Translation)\n",
    "    - [Text Generation](#Text-Generation)\n",
    "- [The Hugging Face Ecosystem](#The-Hugging-Face-Ecosystem)\n",
    "    - [Hugging Face Tokenizers](#Hugging-Face-Tokenizers)\n",
    "    - [Hugging Face Datasets](#Hugging-Face-Datasets)\n",
    "    - [Hugging Face Accelerate](#Hugging-Face-Accelerate)\n",
    "    - [Main Challenges with Transformers](#Main-Challenges-with-Transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd54bcf",
   "metadata": {},
   "source": [
    "# Hello Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4748ca61",
   "metadata": {},
   "source": [
    "## A Tour of Transformer Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4699d22b",
   "metadata": {},
   "source": [
    "\n",
    "Cada tarea de PNL comienza con un fragmento de texto, como los siguientes comentarios inventados de los clientes sobre un determinado sitio en línea orden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dd79566",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Dear Amazon, last week I ordered an Optimus Prime action figure\n",
    "from your online store in Germany. Unfortunately, when I opened the package,\n",
    "I discovered to my horror that I had been sent an action figure of Megatron\n",
    "instead! As a lifelong enemy of the Decepticons, I hope you can understand my\n",
    "dilemma. To resolve the issue, I demand an exchange of Megatron for the\n",
    "Optimus Prime figure I ordered. Enclosed are copies of my records concerning\n",
    "this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35c3e50",
   "metadata": {},
   "source": [
    "Dependiendo de su aplicación, el texto con el que esté trabajando podría ser un contrato legal, una descripción del producto o algo completamente diferente. En el caso de los comentarios de los clientes, probablemente le gustaría saber si los comentarios son `positivos` o `negativos`.\n",
    "\n",
    "Esta tarea se denomina `análisis de sentimientos` y forma parte del tema más amplio de clasificación de textos que exploraremos en el capítulo 2. Por ahora, echemos un `vistazo` a lo que `se necesita` para extraer el sentimiento de nuestro texto usando Transformers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a005f273",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ce8a6b",
   "metadata": {},
   "source": [
    "En Transformers, creamos una instancia de `pipeline` llamando a la función `pipeline()` y proporcionando el nombre de la tarea en la que estamos interesados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa79a563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 11:26:02.708154: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749831962.723179   20001 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749831962.728173   20001 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749831962.740393   20001 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749831962.740408   20001 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749831962.740410   20001 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749831962.740411   20001 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-13 11:26:02.744343: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# pip install ipywidgets\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"text-classification\", framework=\"pt\")  # framework=\"pt\" fuerza PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74edf85b",
   "metadata": {},
   "source": [
    "¡Ahora que tenemos nuestro pipeline, generemos algunas predicciones! Cada pipeline toma una cadena de texto (o una lista de cadenas) como entrada y devuelve un lista de predicciones. Cada predicción es un diccionario de Python, por lo que podemos usar Pandas para mostrarlos los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d035a180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.901546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label     score\n",
       "0  NEGATIVE  0.901546"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "outputs = classifier(text)\n",
    "pd.DataFrame(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1804852e",
   "metadata": {},
   "source": [
    "En este caso, el modelo confía mucho en que el texto tiene un sentimiento negativo, lo cual tiene sentido dado que estamos lidiando con una queja de un ¡cliente enojado! Tenga en cuenta que para las tareas de análisis de sentimientos, la canalización solo devuelve una de las etiquetas POSITIVAS o NEGATIVAS, ya que la otra puede ser inferido calculando 1-Score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68676fdc",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f1f71c",
   "metadata": {},
   "source": [
    "`Predecir el sentimiento de los comentarios de los clientes es un buen primer paso`, pero a menudo desea saber si los comentarios fueron sobre un `artículo o servicio en particular`. En NLP, los objetos del mundo real como productos, lugares y personas se denominan `named entities`, y extraerlos del texto se denomina `named entity recognition(NER)`.\n",
    "Podemos aplicar NER cargando el pipeline correspondiente y enviándole la reseña de nuestros clientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a31a2c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d9be959fe564fd791e739896b10c28d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c7402da5cda4f6c96b01a9177820e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4961ef961b684b96b23b0bfcebe4d3bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_group</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.879010</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.990859</td>\n",
       "      <td>Optimus Prime</td>\n",
       "      <td>36</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.999755</td>\n",
       "      <td>Germany</td>\n",
       "      <td>90</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.556568</td>\n",
       "      <td>Mega</td>\n",
       "      <td>208</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.590257</td>\n",
       "      <td>##tron</td>\n",
       "      <td>212</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.669692</td>\n",
       "      <td>Decept</td>\n",
       "      <td>253</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>##icons</td>\n",
       "      <td>259</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.775361</td>\n",
       "      <td>Megatron</td>\n",
       "      <td>350</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.987854</td>\n",
       "      <td>Optimus Prime</td>\n",
       "      <td>367</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.812096</td>\n",
       "      <td>Bumblebee</td>\n",
       "      <td>502</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_group     score           word  start  end\n",
       "0          ORG  0.879010         Amazon      5   11\n",
       "1         MISC  0.990859  Optimus Prime     36   49\n",
       "2          LOC  0.999755        Germany     90   97\n",
       "3         MISC  0.556568           Mega    208  212\n",
       "4          PER  0.590257         ##tron    212  216\n",
       "5          ORG  0.669692         Decept    253  259\n",
       "6         MISC  0.498350        ##icons    259  264\n",
       "7         MISC  0.775361       Megatron    350  358\n",
       "8         MISC  0.987854  Optimus Prime    367  380\n",
       "9          PER  0.812096      Bumblebee    502  511"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tagger = pipeline(\"ner\", framework=\"pt\",aggregation_strategy=\"simple\")\n",
    "outputs = ner_tagger(text)\n",
    "pd.DataFrame(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796768f0",
   "metadata": {},
   "source": [
    "Puede ver que la pipeline detectó todas las entidades y también asignó una categoría como ORG (organización), LOC (ubicación) o PER (persona) a cada uno de ellos. Aquí usamos el argumento `aggregation_strategy` para agrupar las palabras de acuerdo con las predicciones del modelo. Por ejemplo, la entidad `\"Optimus Prime\"` se compone de dos palabras, pero se le asigna una sola categoría: MISC (miscelánea). `Los puntajes nos dicen cuán confiado estaba el modelo con respecto a las entidades que identificó`. Podemos ver que tenía menos confianza en \"Decepticons\" y la primera aparición de \"Megatron\", los cuales no logró agrupar como `single entity`.\n",
    "\n",
    "Extraer todas las entidades nombradas en un texto es bueno, pero a veces nos gustaría hacer preguntas más específicas. Aquí es donde podemos usar la respuesta a preguntas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5253dbae",
   "metadata": {},
   "source": [
    "## Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e7dc5b",
   "metadata": {},
   "source": [
    "En `question answering`, proporcionamos al modelo un pasaje de texto llamado `contexto`, junto con una pregunta cuya respuesta nos gustaría extraer. Luego, el modelo devuelve el intervalo de texto correspondiente a la respuesta. Veamos qué obtenemos cuando hacemos una pregunta específica sobre los comentarios de nuestros clientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af6928e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ea393d806b4f13a42cb8e968a2b11c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c4c71077e9b4322adca66a922ec2118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eed4c69e8ae4b0daa9d6057bc0603d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aa3a479a54e434aac4427ee41405b00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6c93a03a21e43fa90c27801b3cfba13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.631292</td>\n",
       "      <td>335</td>\n",
       "      <td>358</td>\n",
       "      <td>an exchange of Megatron</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score  start  end                   answer\n",
       "0  0.631292    335  358  an exchange of Megatron"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = pipeline(\"question-answering\",framework=\"pt\")\n",
    "question = \"What does the customer want?\"\n",
    "outputs = reader(question=question, context=text)\n",
    "pd.DataFrame([outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e520f014",
   "metadata": {},
   "source": [
    "Podemos ver que junto con la respuesta, la pipeline también devolvió números enteros de inicio y finalización que corresponden a los índices de caracteres donde se encuentra la respuesta se encontró (al igual que con el etiquetado NER). Hay varios tipos de respuestas a preguntas que investigaremos en el capítulo 7, pero este tipo en particular se llama `extractive question answering` porque la respuesta se extrae directamente del texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e56d47",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43dd45a",
   "metadata": {},
   "source": [
    "`El objetivo del resumen de texto (summarization) es tomar un texto largo como entrada y generar una versión corta con todos los hechos relevantes`. Esta es una tarea mucho más complicada que las anteriores ya que requiere que el modelo genere texto coherente. En lo que debería ser un patrón familiar por ahora, podemos crear una instancia de una pipeline de resumen de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3dd1b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e615a2ab057942c4ad650ce3dee8a145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce0433bd104b41a889eb7bc7d0481fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d173b1a5509749b697894c5023779564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50febf60190446699934af1c79986810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd1883e9e58f4a89a7cd046093b9b264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8876c61ec184467854fb0aebc57eee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Your min_length=56 must be inferior than your max_length=45.\n",
      "/home/luisgarcia/anaconda3/envs/NLP/lib/python3.12/site-packages/transformers/generation/utils.py:1633: UserWarning: Unfeasible length constraints: `min_length` (56) is larger than the maximum possible length (45). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bumblebee ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead.\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\",framework=\"pt\")\n",
    "outputs = summarizer(text, max_length=45, clean_up_tokenization_spaces=True)\n",
    "print(outputs[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61f68a0",
   "metadata": {},
   "source": [
    "`¡Este resumen no es tan malo!` Aunque se han copiado partes del texto original, el modelo pudo capturar la esencia del problema y identifique correctamente que \"Bumblebee\" (que aparecía al final) fue el autor de la denuncia. \n",
    "En este ejemplo, también puede ver que pasamos algunos argumentos de palabras clave como `max_length` y `clean_up_tokenization_spaces` a la pipeline; estos permiten ajustar las salidas en tiempo de ejecución.\n",
    "\n",
    "Pero, ¿qué sucede cuando recibes comentarios en un idioma que no entiendes? Puede usar el traductor de Google, o puede usar tu propio transformador para traducirlo por ti."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31db4977",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2559cbc",
   "metadata": {},
   "source": [
    "Al igual que el resumen, la traducción es una tarea en la que el resultado consiste en texto generado. Usemos una pipeline de traducción para traducir un texto del Inglés al Alemán:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a4b1836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed144d8e05024468905bd9f5ad4e4bec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/768k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d3e0c2b672452bbf90615e9b62974c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/797k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a58986e5994a028dd2536dde71aa94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisgarcia/anaconda3/envs/NLP/lib/python3.12/site-packages/transformers/models/marian/tokenization_marian.py:177: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sehr geehrter Amazon, letzte Woche habe ich eine Optimus Prime Action Figur aus Ihrem Online-Shop in Deutschland bestellt. Leider, als ich das Paket öffnete, entdeckte ich zu meinem Entsetzen, dass ich stattdessen eine Action Figur von Megatron geschickt worden war! Als lebenslanger Feind der Decepticons, Ich hoffe, Sie können mein Dilemma verstehen. Um das Problem zu lösen, Ich fordere einen Austausch von Megatron für die Optimus Prime Figur habe ich bestellt. Eingeschlossen sind Kopien meiner Aufzeichnungen über diesen Kauf. Ich erwarte, von Ihnen bald zu hören. Aufrichtig, Bumblebee.\n"
     ]
    }
   ],
   "source": [
    "#pip install sentencepiece\n",
    "translator = pipeline(\"translation_en_to_de\",framework=\"pt\",model=\"Helsinki-NLP/opus-mt-en-de\")\n",
    "outputs = translator(text, clean_up_tokenization_spaces=True, min_length=100)\n",
    "print(outputs[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60975d2e",
   "metadata": {},
   "source": [
    "Nuevamente, el modelo produjo una traducción muy buena que usa correctamente los pronombres formales del alemán, como \"Ihrem\" y \" Sie.” \n",
    "Aquí también hemos mostrado cómo puede anular el modelo predeterminado en la pipeline para elegir el mejor para su aplicación, y puede encontrar modelos para miles de pares de idiomas en Hugging Face Hub. Antes de dar un paso atrás y analizar todo el Hugging Face Hub, examinemos una última aplicación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c224b3",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fe7fe1",
   "metadata": {},
   "source": [
    "Digamos que le gustaría poder proporcionar respuestas más rápidas a los comentarios de los clientes al tener acceso a una función de autocompletar. Con un modelo de generación de texto, puede hacer esto de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "581523e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9480b7899ae749ee8b4288af1712f0e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb9cc3e11404154848b66756ae930ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82534c48d50044f891297b180451d1ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18084bf1f6ce4e748bd1b28e316abeee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abf8473369264acc81a0ae02baf3c890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a905d643883847369a812894b75af66c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2c3db873e444f0bf8ac05b915a10bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear Amazon, last week I ordered an Optimus Prime action figure\n",
      "from your online store in Germany. Unfortunately, when I opened the package,\n",
      "I discovered to my horror that I had been sent an action figure of Megatron\n",
      "instead! As a lifelong enemy of the Decepticons, I hope you can understand my\n",
      "dilemma. To resolve the issue, I demand an exchange of Megatron for the\n",
      "Optimus Prime figure I ordered. Enclosed are copies of my records concerning\n",
      "this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\n",
      "\n",
      "Customer service response:\n",
      "Dear Bumblebee, I am sorry to hear that your order was mixed up. You ordered multiple sets of Megatron figures.\n",
      "\n",
      "I apologize to the customer for confusion. I realize that you made your mistake and I will not be responsible for it.\n",
      "\n",
      "If you need a refund of your order, please contact me at bumblebee@gmail.com. I will look into the issue and try to resolve it.\n",
      "\n",
      "If you already have a copy of your order, please contact me. I will look into it and try to resolve it.\n",
      "\n",
      "Thank you for your patience.\n",
      "\n",
      "Sincerely,\n",
      "\n",
      "Customer service response:\n",
      "\n",
      "Dear Bumblebee, I received your message within 24 hours. I will continue to work with you to resolve this matter.\n",
      "\n",
      "If you are still having issues, please contact me at bumblebee@gmail.com. I will look into the issue and try to resolve it.\n",
      "\n",
      "Thank you for your patience.\n",
      "\n",
      "Sincerely, Bumblebee\n",
      "\n",
      "Sincerely,\n",
      "\n",
      "Customer service response:\n",
      "\n",
      "Dear Bumblebee, Thanks for your service. My question is, did you decide to order the Optimus Prime action figure? I have been told by one of the distributors that a set will be shipped to you, but what are the details of the shipping\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\",framework=\"pt\")\n",
    "response = \"Dear Bumblebee, I am sorry to hear that your order was mixed up.\"\n",
    "prompt = text + \"\\n\\nCustomer service response:\\n\" + response\n",
    "outputs = generator(prompt, max_length=200)\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcce335e",
   "metadata": {},
   "source": [
    "OK, tal vez no querríamos usar esta finalización para calmar a Bumblebee, pero entiendes la idea general.\n",
    "\n",
    "Ahora que has visto algunas aplicaciones geniales de los modelos de transformadores, es posible que se pregunte dónde ocurre el entrenamiento. Todos los modelos que hemos utilizado en este capítulo están disponibles públicamente y ya están ajustados para la tarea en cuestión. Sin embargo, en general, querrá ajustar los modelos en sus propios datos, y en los siguientes capítulos aprenderá a hacer precisamente eso.\n",
    "\n",
    "Pero entrenar un modelo es solo una pequeña parte de cualquier proyecto de NLP: poder procesar datos de manera eficiente, compartir resultados con colegas y hacer que su el trabajo reproducible también es un componente clave. Afortunadamente, Transformers está rodeado por un gran ecosistema de herramientas útiles que admiten gran parte del flujo de trabajo moderno de aprendizaje automático. Echemos un vistazo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c423df6",
   "metadata": {},
   "source": [
    "# The Hugging Face Ecosystem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251a713f",
   "metadata": {},
   "source": [
    "El ecosistema Hugging Face consta principalmente de dos partes: una familia de bibliotecas y el HUB.\n",
    "\n",
    "Las bibliotecas proporcionan el código, mientras que el HUB proporciona los pesos de los modelo previamente entrenados, los conjuntos de datos, los scripts para las métricas de evaluación y más. En esta sección veremos brevemente los distintos componentes. Vamos a Transformers, como ya lo hemos discutido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc17647e",
   "metadata": {},
   "source": [
    "## Hugging Face Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9472ebcf",
   "metadata": {},
   "source": [
    "Detrás de cada uno de los ejemplos de pipelines que hemos visto en este capítulo hay un paso de tokenización que divide el texto en piezas más pequeñas llamadas tokens. Veremos cómo funciona esto en detalle en el Capítulo 2, pero por ahora es suficiente entender que los `tokens pueden ser palabras, partes de palabras o simplemente caracteres como la puntuación`.\n",
    "\n",
    "Los modelos de transformadores se entrenan en representaciones numéricas de estos tokens, `así que hacer bien este paso es bastante importante para todo el proyecto de PLN!`\n",
    "\n",
    "`Tokenizers` ofrece muchas estrategias de tokenización y es extremadamente rápido en la tokenización de texto gracias a su backend en Rust. `También se encarga de todos los pasos de pre y postprocesamiento`, como normalizar las entradas y transformar las salidas del modelo al formato requerido. Con Tokenizers, podemos cargar un tokenizador de la misma manera que podemos cargar pesos de modelos preentrenados con Transformers.\n",
    "\n",
    "Necesitamos un conjunto de datos y métricas para entrenar y evaluar modelos, así que echemos un vistazo a Datasets, que se encarga de ese aspecto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a008a1b",
   "metadata": {},
   "source": [
    "## Hugging Face Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627141af",
   "metadata": {},
   "source": [
    "Cargar, procesar y almacenar conjuntos de datos puede ser un proceso engorroso, especialmente cuando los conjuntos de datos son demasiado grandes para caber en la RAM de su laptop. Además, generalmente necesita implementar varios scripts para descargar los datos y transformarlos en un formato estándar.\n",
    "\n",
    "Los conjuntos de datos simplifican este proceso al proporcionar una interfaz estándar para miles de conjuntos de datos que se pueden encontrar en el Hub. También ofrece almacenamiento en caché inteligente (para que no tengas que repetir tu preprocesamiento cada vez que ejecutes tu código) y evita las limitaciones de RAM al aprovechar un mecanismo especial llamado called memory mapping que almacena el contenido de un archivo en la memoria virtual y permite que múltiples procesos modifiquen un archivo de manera más eficiente. La biblioteca también es interoperable con marcos populares como Pandas y NumPy, por lo que no tienes que abandonar la comodidad de tus herramientas favoritas para manejar datos.\n",
    "\n",
    "¡Con las bibliotecas Transformers, Tokenizers y Datasets tenemos todo lo que necesitamos para entrenar nuestros propios modelos transformer! Sin embargo, como veremos en el Capítulo 10, hay situaciones en las que necesitamos un control detallado sobre el bucle de entrenamiento. Ahí es donde entra en juego la última biblioteca del ecosistema: Accelerate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dba1ce1",
   "metadata": {},
   "source": [
    "## Hugging Face Accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadcfca7",
   "metadata": {},
   "source": [
    "Si alguna vez has tenido que escribir tu propio script de entrenamiento en PyTorch, es probable que hayas tenido algunos dolores de cabeza al intentar trasladar el código que se ejecuta en tu portátil al código que se ejecuta en el clúster de tu organización. Accelerate añade una capa de abstracción a tus bucles de entrenamiento normales que se encarga de toda la lógica personalizada necesaria para la infraestructura de entrenamiento. Esto literalmente acelera tu flujo de trabajo al simplificar el cambio de infraestructura cuando es necesario.\n",
    "\n",
    "Esto resume los componentes clave del ecosistema de código abierto de Hugging Face. Pero antes de concluir este capítulo, echemos un vistazo a algunos de los desafíos comunes que surgen al intentar implementar transformadores en el mundo real."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d417d3d",
   "metadata": {},
   "source": [
    "## Main Challenges with Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec1a0ec",
   "metadata": {},
   "source": [
    "En este capítulo hemos obtenido una visión general de la amplia gama de tareas de PNL que se pueden abordar con modelos de transformadores. Al leer los titulares de los medios, a veces puede parecer que sus capacidades son ilimitadas. Sin embargo, a pesar de su utilidad, `los transformadores están lejos de ser una solución mágica`. Aquí hay algunos desafíos asociados con ellos que exploraremos a lo largo del libro:\n",
    "\n",
    "- `Lenguaje`: La investigación en PLN está dominada por el idioma inglés. Hay varios modelos para otros idiomas, pero es más difícil encontrar modelos preentrenados para idiomas raros o con pocos recursos. En el Capítulo 4, exploraremos los transformadores multilingües y su capacidad para realizar transferencias cruzadas en cero disparos.\n",
    "- `Disponibilidad de datos`: Aunque podemos utilizar el aprendizaje por transferencia para reducir drásticamente la cantidad de datos de entrenamiento etiquetados que nuestros modelos necesitan, sigue siendo mucho en comparación con lo que un humano necesita para realizar la tarea. `Abordar escenarios donde tienes pocos o ningún dato etiquetado es el tema del Capítulo 9`.\n",
    "- `Trabajar con documentos largos`: La autoatención funciona extremadamente bien en textos de párrafo, pero se vuelve muy costosa cuando pasamos a textos más largos como documentos completos. Se discuten enfoques para mitigar esto en el Capítulo 11.\n",
    "- `Opacidad`: Al igual que otros modelos de aprendizaje profundo, los transformadores son en gran medida opacos. Es difícil o imposible desentrañar \"por qué\" un modelo hizo una cierta predicción. Este es un desafío especialmente difícil cuando estos modelos se utilizan para tomar decisiones críticas. Exploraremos algunas maneras de investigar los errores de los modelos de transformadores en los Capítulos 2 y 4.\n",
    "- `Sesgo`: Los modelos de transformadores se preentrenan predominantemente con datos de texto de internet. Esto imprime todos los sesgos que están presentes en los datos en los modelos. Asegurarse de que estos no sean racistas, sexistas es una tarea difícil. Discutimos algunos de estos problemas con más detalle en el Capítulo 10.\n",
    "\n",
    "Aunque desalentadores, muchos de estos desafíos se pueden superar. Además de en los capítulos específicos mencionados, tocaremos estos temas en casi todos los capítulos que vienen.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6591bc9",
   "metadata": {},
   "source": [
    "# Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8a812d",
   "metadata": {},
   "source": [
    "¡Esperemos que, para este momento, estés emocionado por aprender cómo comenzar a entrenar e integrar estos versátiles modelos en tus propias aplicaciones! Has visto en este capítulo que con solo unas pocas líneas de código puedes utilizar modelos de última generación para clasificación, reconocimiento de entidades nombradas, respuesta a preguntas, traducción y resumen, pero esto es realmente solo la \"punta del iceberg.\"\n",
    "\n",
    "En los siguientes capítulos aprenderás cómo adaptar transformadores a una amplia gama de casos de uso, como la construcción de un clasificador de texto, o un modelo ligero para producción, o incluso entrenar un modelo de lenguaje desde cero. Adoptaremos un enfoque práctico, lo que significa que para cada concepto que se trate habrá código acompañante que podrás ejecutar en Google Colab o en tu propia máquina con GPU.\n",
    "\n",
    "Ahora que estamos armados con los conceptos básicos detrás de los transformadores, es hora de ensuciarnos las manos con nuestra primera aplicación: la clasificación de texto. ¡Ese es el tema del próximo capítulo!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
